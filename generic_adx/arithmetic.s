// Code generated by command: go run main.go -output ./generic_adx -opt D -arch ADX. DO NOT EDIT.

#include "textflag.h"

// func cpy2(dst *[2]uint64, src *[2]uint64)
TEXT ·cpy2(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	RET

// func eq2(a *[2]uint64, b *[2]uint64) bool
TEXT ·eq2(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp2(a *[2]uint64, b *[2]uint64) int8
TEXT ·cmp2(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add2(c *[2]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64)
TEXT ·add2(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R8
	SUBQ (SI), R8
	MOVQ DX, R9
	SBBQ 8(SI), R9
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R8, CX
	MOVQ    CX, (DI)
	CMOVQCC R9, DX
	MOVQ    DX, 8(DI)
	RET

// func addn2(a *[2]uint64, b *[2]uint64) uint64
TEXT ·addn2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double2(c *[2]uint64, a *[2]uint64, p *[2]uint64)
TEXT ·double2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R8
	SUBQ (SI), R8
	MOVQ DX, R9
	SBBQ 8(SI), R9
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R8, CX
	MOVQ    CX, (DI)
	CMOVQCC R9, DX
	MOVQ    DX, 8(DI)
	RET

// func sub2(c *[2]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64)
TEXT ·sub2(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R8
	CMOVQCC AX, R8
	MOVQ    8(SI), R9
	CMOVQCC AX, R9

	// |
	MOVQ c+0(FP), DI
	ADDQ R8, CX
	MOVQ CX, (DI)
	ADCQ R9, DX
	MOVQ DX, 8(DI)
	RET

// func subn2(a *[2]uint64, b *[2]uint64) uint64
TEXT ·subn2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg2(c *[2]uint64, a *[2]uint64, p *[2]uint64)
TEXT ·_neg2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	RET

// func mul_two_2(a *[2]uint64)
TEXT ·mul_two_2(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RET

// func div_two_2(a *[2]uint64)
TEXT ·div_two_2(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul2(c *[4]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64, inp uint64)
TEXT ·mul2(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8
	ADCQ  $0x00, R9

	// |
	// |
	MOVQ 8(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ p+24(FP), R15

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9
	ADOXQ BX, R9
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, SI
	ADOXQ BX, SI
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R8, BX

	// | 

/* reduction 				*/

	MOVQ R9, AX
	SUBQ (R15), AX
	MOVQ SI, DX
	SBBQ 8(R15), DX
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R9
	MOVQ    R9, (BX)
	CMOVQCC DX, SI
	MOVQ    SI, 8(BX)
	RET

	// | 

/* end 				*/


// func cpy3(dst *[3]uint64, src *[3]uint64)
TEXT ·cpy3(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	RET

// func eq3(a *[3]uint64, b *[3]uint64) bool
TEXT ·eq3(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp3(a *[3]uint64, b *[3]uint64) int8
TEXT ·cmp3(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add3(c *[3]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64)
TEXT ·add3(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R9
	SUBQ (SI), R9
	MOVQ DX, R10
	SBBQ 8(SI), R10
	MOVQ R8, R11
	SBBQ 16(SI), R11
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R9, CX
	MOVQ    CX, (DI)
	CMOVQCC R10, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R11, R8
	MOVQ    R8, 16(DI)
	RET

// func addn3(a *[3]uint64, b *[3]uint64) uint64
TEXT ·addn3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double3(c *[3]uint64, a *[3]uint64, p *[3]uint64)
TEXT ·double3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R9
	SUBQ (SI), R9
	MOVQ DX, R10
	SBBQ 8(SI), R10
	MOVQ R8, R11
	SBBQ 16(SI), R11
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R9, CX
	MOVQ    CX, (DI)
	CMOVQCC R10, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R11, R8
	MOVQ    R8, 16(DI)
	RET

// func sub3(c *[3]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64)
TEXT ·sub3(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R9
	CMOVQCC AX, R9
	MOVQ    8(SI), R10
	CMOVQCC AX, R10
	MOVQ    16(SI), R11
	CMOVQCC AX, R11

	// |
	MOVQ c+0(FP), DI
	ADDQ R9, CX
	MOVQ CX, (DI)
	ADCQ R10, DX
	MOVQ DX, 8(DI)
	ADCQ R11, R8
	MOVQ R8, 16(DI)
	RET

// func subn3(a *[3]uint64, b *[3]uint64) uint64
TEXT ·subn3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg3(c *[3]uint64, a *[3]uint64, p *[3]uint64)
TEXT ·_neg3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	RET

// func mul_two_3(a *[3]uint64)
TEXT ·mul_two_3(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RET

// func div_two_3(a *[3]uint64)
TEXT ·div_two_3(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul3(c *[6]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64, inp uint64)
TEXT ·mul3(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 16(DI), AX, R10
	ADCXQ AX, R9
	ADCQ  $0x00, R10

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R11, R11

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADOXQ R11, R11
	ADCXQ BX, R11

	// |
	// |
	MOVQ 16(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ p+24(FP), R15

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10
	ADOXQ BX, R10
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11
	ADOXQ BX, R11
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R8, R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, SI
	ADOXQ BX, SI
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R9, BX

	// | 

/* reduction 				*/

	MOVQ R10, AX
	SUBQ (R15), AX
	MOVQ R11, DX
	SBBQ 8(R15), DX
	MOVQ SI, DI
	SBBQ 16(R15), DI
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R10
	MOVQ    R10, (BX)
	CMOVQCC DX, R11
	MOVQ    R11, 8(BX)
	CMOVQCC DI, SI
	MOVQ    SI, 16(BX)
	RET

	// | 

/* end 				*/


// func cpy4(dst *[4]uint64, src *[4]uint64)
TEXT ·cpy4(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	RET

// func eq4(a *[4]uint64, b *[4]uint64) bool
TEXT ·eq4(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp4(a *[4]uint64, b *[4]uint64) int8
TEXT ·cmp4(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64)
TEXT ·add4(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R10
	SUBQ (SI), R10
	MOVQ DX, R11
	SBBQ 8(SI), R11
	MOVQ R8, R12
	SBBQ 16(SI), R12
	MOVQ R9, R13
	SBBQ 24(SI), R13
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R10, CX
	MOVQ    CX, (DI)
	CMOVQCC R11, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R12, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R13, R9
	MOVQ    R9, 24(DI)
	RET

// func addn4(a *[4]uint64, b *[4]uint64) uint64
TEXT ·addn4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double4(c *[4]uint64, a *[4]uint64, p *[4]uint64)
TEXT ·double4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R10
	SUBQ (SI), R10
	MOVQ DX, R11
	SBBQ 8(SI), R11
	MOVQ R8, R12
	SBBQ 16(SI), R12
	MOVQ R9, R13
	SBBQ 24(SI), R13
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R10, CX
	MOVQ    CX, (DI)
	CMOVQCC R11, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R12, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R13, R9
	MOVQ    R9, 24(DI)
	RET

// func sub4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64)
TEXT ·sub4(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R10
	CMOVQCC AX, R10
	MOVQ    8(SI), R11
	CMOVQCC AX, R11
	MOVQ    16(SI), R12
	CMOVQCC AX, R12
	MOVQ    24(SI), R13
	CMOVQCC AX, R13

	// |
	MOVQ c+0(FP), DI
	ADDQ R10, CX
	MOVQ CX, (DI)
	ADCQ R11, DX
	MOVQ DX, 8(DI)
	ADCQ R12, R8
	MOVQ R8, 16(DI)
	ADCQ R13, R9
	MOVQ R9, 24(DI)
	RET

// func subn4(a *[4]uint64, b *[4]uint64) uint64
TEXT ·subn4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg4(c *[4]uint64, a *[4]uint64, p *[4]uint64)
TEXT ·_neg4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	RET

// func mul_two_4(a *[4]uint64)
TEXT ·mul_two_4(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RET

// func div_two_4(a *[4]uint64)
TEXT ·div_two_4(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul4(c *[8]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64, inp uint64)
TEXT ·mul4(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 16(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 24(DI), AX, R11
	ADCXQ AX, R10
	ADCQ  $0x00, R11

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R12, R12

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADOXQ R12, R12
	ADCXQ BX, R12

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R13, R13

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADOXQ R13, R13
	ADCXQ BX, R13

	// |
	// |
	MOVQ 24(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ p+24(FP), R15

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11
	ADOXQ BX, R11
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12
	ADOXQ BX, R12
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R8, R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13
	ADOXQ BX, R13
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R9, R9

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, SI
	ADOXQ BX, SI
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R10, BX

	// | 

/* reduction 				*/

	MOVQ R11, AX
	SUBQ (R15), AX
	MOVQ R12, DX
	SBBQ 8(R15), DX
	MOVQ R13, DI
	SBBQ 16(R15), DI
	MOVQ SI, R8
	SBBQ 24(R15), R8
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R11
	MOVQ    R11, (BX)
	CMOVQCC DX, R12
	MOVQ    R12, 8(BX)
	CMOVQCC DI, R13
	MOVQ    R13, 16(BX)
	CMOVQCC R8, SI
	MOVQ    SI, 24(BX)
	RET

	// | 

/* end 				*/


// func cpy5(dst *[5]uint64, src *[5]uint64)
TEXT ·cpy5(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	RET

// func eq5(a *[5]uint64, b *[5]uint64) bool
TEXT ·eq5(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp5(a *[5]uint64, b *[5]uint64) int8
TEXT ·cmp5(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64)
TEXT ·add5(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R11
	SUBQ (SI), R11
	MOVQ DX, R12
	SBBQ 8(SI), R12
	MOVQ R8, R13
	SBBQ 16(SI), R13
	MOVQ R9, R14
	SBBQ 24(SI), R14
	MOVQ R10, R15
	SBBQ 32(SI), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R11, CX
	MOVQ    CX, (DI)
	CMOVQCC R12, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R13, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 24(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 32(DI)
	RET

// func addn5(a *[5]uint64, b *[5]uint64) uint64
TEXT ·addn5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double5(c *[5]uint64, a *[5]uint64, p *[5]uint64)
TEXT ·double5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R11
	SUBQ (SI), R11
	MOVQ DX, R12
	SBBQ 8(SI), R12
	MOVQ R8, R13
	SBBQ 16(SI), R13
	MOVQ R9, R14
	SBBQ 24(SI), R14
	MOVQ R10, R15
	SBBQ 32(SI), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R11, CX
	MOVQ    CX, (DI)
	CMOVQCC R12, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R13, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 24(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 32(DI)
	RET

// func sub5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64)
TEXT ·sub5(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R11
	CMOVQCC AX, R11
	MOVQ    8(SI), R12
	CMOVQCC AX, R12
	MOVQ    16(SI), R13
	CMOVQCC AX, R13
	MOVQ    24(SI), R14
	CMOVQCC AX, R14
	MOVQ    32(SI), R15
	CMOVQCC AX, R15

	// |
	MOVQ c+0(FP), DI
	ADDQ R11, CX
	MOVQ CX, (DI)
	ADCQ R12, DX
	MOVQ DX, 8(DI)
	ADCQ R13, R8
	MOVQ R8, 16(DI)
	ADCQ R14, R9
	MOVQ R9, 24(DI)
	ADCQ R15, R10
	MOVQ R10, 32(DI)
	RET

// func subn5(a *[5]uint64, b *[5]uint64) uint64
TEXT ·subn5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg5(c *[5]uint64, a *[5]uint64, p *[5]uint64)
TEXT ·_neg5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	RET

// func mul_two_5(a *[5]uint64)
TEXT ·mul_two_5(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RET

// func div_two_5(a *[5]uint64)
TEXT ·div_two_5(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul5(c *[10]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64, inp uint64)
TEXT ·mul5(SB), NOSPLIT, $8-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 16(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 24(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 32(DI), AX, R12
	ADCXQ AX, R11
	ADCQ  $0x00, R12

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R13, R13

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADOXQ R13, R13
	ADCXQ BX, R13

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R14, R14

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ R14, R14
	ADCXQ BX, R14

	// |
	// |
	MOVQ 24(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 32(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ SI, (SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12
	ADOXQ BX, R12
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13
	ADOXQ BX, R13
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R8, R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, R14
	ADOXQ BX, R14
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R9, R9

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, R14

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R14
	ADCXQ DI, R15
	ADOXQ BX, R15
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R10, R10
	MOVQ  (SP), R10

	// |
	// |
	MOVQ  R11, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, R14

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R14
	ADCXQ DI, R15

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R15
	ADCXQ DI, R10
	ADOXQ BX, R10
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R11, BX

	// | 

/* reduction 				*/

	MOVQ R12, AX
	SUBQ (SI), AX
	MOVQ R13, DX
	SBBQ 8(SI), DX
	MOVQ R14, DI
	SBBQ 16(SI), DI
	MOVQ R15, R8
	SBBQ 24(SI), R8
	MOVQ R10, R9
	SBBQ 32(SI), R9
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R12
	MOVQ    R12, (BX)
	CMOVQCC DX, R13
	MOVQ    R13, 8(BX)
	CMOVQCC DI, R14
	MOVQ    R14, 16(BX)
	CMOVQCC R8, R15
	MOVQ    R15, 24(BX)
	CMOVQCC R9, R10
	MOVQ    R10, 32(BX)
	RET

	// | 

/* end 				*/


// func cpy6(dst *[6]uint64, src *[6]uint64)
TEXT ·cpy6(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	RET

// func eq6(a *[6]uint64, b *[6]uint64) bool
TEXT ·eq6(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp6(a *[6]uint64, b *[6]uint64) int8
TEXT ·cmp6(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64)
TEXT ·add6(SB), NOSPLIT, $16-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R12
	SUBQ (SI), R12
	MOVQ DX, R13
	SBBQ 8(SI), R13
	MOVQ R8, R14
	SBBQ 16(SI), R14
	MOVQ R9, R15
	SBBQ 24(SI), R15
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, (SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 8(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, CX
	MOVQ    CX, (DI)
	CMOVQCC R13, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R14, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R15, R9
	MOVQ    R9, 24(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 8(SP), R11
	MOVQ    R11, 40(DI)
	RET

// func addn6(a *[6]uint64, b *[6]uint64) uint64
TEXT ·addn6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double6(c *[6]uint64, a *[6]uint64, p *[6]uint64)
TEXT ·double6(SB), NOSPLIT, $16-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R12
	SUBQ (SI), R12
	MOVQ DX, R13
	SBBQ 8(SI), R13
	MOVQ R8, R14
	SBBQ 16(SI), R14
	MOVQ R9, R15
	SBBQ 24(SI), R15
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, (SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 8(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, CX
	MOVQ    CX, (DI)
	CMOVQCC R13, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R14, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R15, R9
	MOVQ    R9, 24(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 8(SP), R11
	MOVQ    R11, 40(DI)
	RET

// func sub6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64)
TEXT ·sub6(SB), NOSPLIT, $16-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R12
	CMOVQCC AX, R12
	MOVQ    8(SI), R13
	CMOVQCC AX, R13
	MOVQ    16(SI), R14
	CMOVQCC AX, R14
	MOVQ    24(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 32(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 8(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R12, CX
	MOVQ CX, (DI)
	ADCQ R13, DX
	MOVQ DX, 8(DI)
	ADCQ R14, R8
	MOVQ R8, 16(DI)
	ADCQ R15, R9
	MOVQ R9, 24(DI)
	ADCQ (SP), R10
	MOVQ R10, 32(DI)
	ADCQ 8(SP), R11
	MOVQ R11, 40(DI)
	RET

// func subn6(a *[6]uint64, b *[6]uint64) uint64
TEXT ·subn6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg6(c *[6]uint64, a *[6]uint64, p *[6]uint64)
TEXT ·_neg6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	RET

// func mul_two_6(a *[6]uint64)
TEXT ·mul_two_6(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RET

// func div_two_6(a *[6]uint64)
TEXT ·div_two_6(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul6(c *[12]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64, inp uint64)
TEXT ·mul6(SB), NOSPLIT, $24-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, CX
	MOVQ  AX, (SP)

	// |
	MULXQ 8(DI), AX, R8
	ADCXQ AX, CX

	// |
	MULXQ 16(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 24(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 32(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 40(DI), AX, R12
	ADCXQ AX, R11
	ADCQ  $0x00, R12

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R13, R13

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8
	MOVQ  CX, 8(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R12
	ADOXQ R13, R13
	ADCXQ BX, R13

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R14, R14

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ R14, R14
	ADCXQ BX, R14

	// |
	// |
	MOVQ 24(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 32(SI), DX
	XORQ CX, CX

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ CX, CX
	ADCXQ BX, CX

	// |
	// |
	MOVQ 40(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, CX
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ (SP), DI
	MOVQ 8(SP), BX
	MOVQ R15, (SP)
	MOVQ CX, 8(SP)
	MOVQ SI, 16(SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  CX, CX
	MOVQ  DI, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, DI
	ADCXQ R15, BX

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, BX
	ADCXQ R15, R8

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12
	ADOXQ CX, R12
	ADCXQ CX, CX
	XORQ  DI, DI

	// |
	// |
	MOVQ  BX, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, BX
	ADCXQ R15, R8

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13
	ADOXQ CX, R13
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  BX, BX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14
	ADOXQ CX, R14
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  R8, R8
	MOVQ  (SP), R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R14
	ADCXQ R15, R8
	ADOXQ CX, R8
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  R9, R9
	MOVQ  8(SP), R9

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R14
	ADCXQ R15, R8

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9
	ADOXQ CX, R9
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  R10, R10
	MOVQ  16(SP), R10

	// |
	// |
	MOVQ  R11, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R14
	ADCXQ R15, R8

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10
	ADOXQ CX, R10
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	ADOXQ R11, CX

	// | 

/* reduction 				*/

	MOVQ R12, DX
	SUBQ (SI), DX
	MOVQ R13, BX
	SBBQ 8(SI), BX
	MOVQ R14, DI
	SBBQ 16(SI), DI
	MOVQ R8, R11
	SBBQ 24(SI), R11
	MOVQ R9, R15
	SBBQ 32(SI), R15
	MOVQ R10, AX
	SBBQ 40(SI), AX
	MOVQ AX, (SP)
	SBBQ $0x00, CX

	// |
	MOVQ    c+0(FP), CX
	CMOVQCC DX, R12
	MOVQ    R12, (CX)
	CMOVQCC BX, R13
	MOVQ    R13, 8(CX)
	CMOVQCC DI, R14
	MOVQ    R14, 16(CX)
	CMOVQCC R11, R8
	MOVQ    R8, 24(CX)
	CMOVQCC R15, R9
	MOVQ    R9, 32(CX)
	CMOVQCC (SP), R10
	MOVQ    R10, 40(CX)
	RET

	// | 

/* end 				*/


// func cpy7(dst *[7]uint64, src *[7]uint64)
TEXT ·cpy7(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	RET

// func eq7(a *[7]uint64, b *[7]uint64) bool
TEXT ·eq7(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp7(a *[7]uint64, b *[7]uint64) int8
TEXT ·cmp7(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64)
TEXT ·add7(SB), NOSPLIT, $32-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R13
	SUBQ (SI), R13
	MOVQ DX, R14
	SBBQ 8(SI), R14
	MOVQ R8, R15
	SBBQ 16(SI), R15
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, (SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 24(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, CX
	MOVQ    CX, (DI)
	CMOVQCC R14, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R15, R8
	MOVQ    R8, 16(DI)
	CMOVQCC (SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 24(SP), R12
	MOVQ    R12, 48(DI)
	RET

// func addn7(a *[7]uint64, b *[7]uint64) uint64
TEXT ·addn7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double7(c *[7]uint64, a *[7]uint64, p *[7]uint64)
TEXT ·double7(SB), NOSPLIT, $32-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R13
	SUBQ (SI), R13
	MOVQ DX, R14
	SBBQ 8(SI), R14
	MOVQ R8, R15
	SBBQ 16(SI), R15
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, (SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 24(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, CX
	MOVQ    CX, (DI)
	CMOVQCC R14, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R15, R8
	MOVQ    R8, 16(DI)
	CMOVQCC (SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 24(SP), R12
	MOVQ    R12, 48(DI)
	RET

// func sub7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64)
TEXT ·sub7(SB), NOSPLIT, $32-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R13
	CMOVQCC AX, R13
	MOVQ    8(SI), R14
	CMOVQCC AX, R14
	MOVQ    16(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 24(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 24(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R13, CX
	MOVQ CX, (DI)
	ADCQ R14, DX
	MOVQ DX, 8(DI)
	ADCQ R15, R8
	MOVQ R8, 16(DI)
	ADCQ (SP), R9
	MOVQ R9, 24(DI)
	ADCQ 8(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 16(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 24(SP), R12
	MOVQ R12, 48(DI)
	RET

// func subn7(a *[7]uint64, b *[7]uint64) uint64
TEXT ·subn7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg7(c *[7]uint64, a *[7]uint64, p *[7]uint64)
TEXT ·_neg7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	RET

// func mul_two_7(a *[7]uint64)
TEXT ·mul_two_7(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RET

// func div_two_7(a *[7]uint64)
TEXT ·div_two_7(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul7(c *[14]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64, inp uint64)
TEXT ·mul7(SB), NOSPLIT, $40-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, CX
	MOVQ  AX, (SP)

	// |
	MULXQ 8(DI), AX, R8
	ADCXQ AX, CX

	// |
	MULXQ 16(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 24(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 32(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 40(DI), AX, R12
	ADCXQ AX, R11

	// |
	MULXQ 48(DI), AX, R13
	ADCXQ AX, R12
	ADCQ  $0x00, R13

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R14, R14

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8
	MOVQ  CX, 8(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ R14, R14
	ADCXQ BX, R14

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9
	MOVQ  R8, 16(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 24(SI), DX
	XORQ CX, CX

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10
	MOVQ  R9, 24(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ CX, CX
	ADCXQ BX, CX

	// |
	// |
	MOVQ 32(SI), DX
	XORQ R8, R8

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, CX
	ADOXQ R8, R8
	ADCXQ BX, R8

	// |
	// |
	MOVQ 40(SI), DX
	XORQ R9, R9

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R8
	ADOXQ R9, R9
	ADCXQ BX, R9

	// |
	// |
	MOVQ 48(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R9
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ (SP), DI
	MOVQ 8(SP), BX
	MOVQ R15, (SP)
	MOVQ 16(SP), R15
	MOVQ CX, 8(SP)
	MOVQ 24(SP), CX
	MOVQ R8, 16(SP)
	MOVQ R9, 24(SP)
	MOVQ SI, 32(SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  R9, R9
	MOVQ  DI, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, DI
	ADCXQ R8, BX

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13
	ADOXQ R9, R13
	ADCXQ R9, R9
	XORQ  DI, DI

	// |
	// |
	MOVQ  BX, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14
	ADOXQ R9, R14
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  BX, BX
	MOVQ  (SP), BX

	// |
	// |
	MOVQ  R15, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX
	ADOXQ R9, BX
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  R15, R15
	MOVQ  8(SP), R15

	// |
	// |
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15
	ADOXQ R9, R15
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  CX, CX
	MOVQ  16(SP), CX

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX
	ADOXQ R9, CX
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  R10, R10
	MOVQ  24(SP), R10

	// |
	// |
	MOVQ  R11, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10
	ADOXQ R9, R10
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  R11, R11
	MOVQ  32(SP), R11

	// |
	// |
	MOVQ  R12, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11
	ADOXQ R9, R11
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	ADOXQ R12, R9

	// | 

/* reduction 				*/

	MOVQ R13, DX
	SUBQ (SI), DX
	MOVQ R14, DI
	SBBQ 8(SI), DI
	MOVQ BX, R8
	SBBQ 16(SI), R8
	MOVQ R15, R12
	SBBQ 24(SI), R12
	MOVQ CX, AX
	SBBQ 32(SI), AX
	MOVQ AX, (SP)
	MOVQ R10, AX
	SBBQ 40(SI), AX
	MOVQ AX, 8(SP)
	MOVQ R11, AX
	SBBQ 48(SI), AX
	MOVQ AX, 16(SP)
	SBBQ $0x00, R9

	// |
	MOVQ    c+0(FP), R9
	CMOVQCC DX, R13
	MOVQ    R13, (R9)
	CMOVQCC DI, R14
	MOVQ    R14, 8(R9)
	CMOVQCC R8, BX
	MOVQ    BX, 16(R9)
	CMOVQCC R12, R15
	MOVQ    R15, 24(R9)
	CMOVQCC (SP), CX
	MOVQ    CX, 32(R9)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 40(R9)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 48(R9)
	RET

	// | 

/* end 				*/


// func cpy8(dst *[8]uint64, src *[8]uint64)
TEXT ·cpy8(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	RET

// func eq8(a *[8]uint64, b *[8]uint64) bool
TEXT ·eq8(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp8(a *[8]uint64, b *[8]uint64) int8
TEXT ·cmp8(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64)
TEXT ·add8(SB), NOSPLIT, $48-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R14
	SUBQ (SI), R14
	MOVQ DX, R15
	SBBQ 8(SI), R15
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, (SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 40(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, CX
	MOVQ    CX, (DI)
	CMOVQCC R15, DX
	MOVQ    DX, 8(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 40(SP), R13
	MOVQ    R13, 56(DI)
	RET

// func addn8(a *[8]uint64, b *[8]uint64) uint64
TEXT ·addn8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double8(c *[8]uint64, a *[8]uint64, p *[8]uint64)
TEXT ·double8(SB), NOSPLIT, $48-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R14
	SUBQ (SI), R14
	MOVQ DX, R15
	SBBQ 8(SI), R15
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, (SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 40(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, CX
	MOVQ    CX, (DI)
	CMOVQCC R15, DX
	MOVQ    DX, 8(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 40(SP), R13
	MOVQ    R13, 56(DI)
	RET

// func sub8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64)
TEXT ·sub8(SB), NOSPLIT, $48-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R14
	CMOVQCC AX, R14
	MOVQ    8(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 16(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 24(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 40(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R14, CX
	MOVQ CX, (DI)
	ADCQ R15, DX
	MOVQ DX, 8(DI)
	ADCQ (SP), R8
	MOVQ R8, 16(DI)
	ADCQ 8(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 16(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 24(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 32(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 40(SP), R13
	MOVQ R13, 56(DI)
	RET

// func subn8(a *[8]uint64, b *[8]uint64) uint64
TEXT ·subn8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg8(c *[8]uint64, a *[8]uint64, p *[8]uint64)
TEXT ·_neg8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	RET

// func mul_two_8(a *[8]uint64)
TEXT ·mul_two_8(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RET

// func div_two_8(a *[8]uint64)
TEXT ·div_two_8(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul8(c *[16]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64, inp uint64)
TEXT ·mul8(SB), NOSPLIT, $56-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, CX
	MOVQ  AX, (SP)

	// |
	MULXQ 8(DI), AX, R8
	ADCXQ AX, CX

	// |
	MULXQ 16(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 24(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 32(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 40(DI), AX, R12
	ADCXQ AX, R11

	// |
	MULXQ 48(DI), AX, R13
	ADCXQ AX, R12

	// |
	MULXQ 56(DI), AX, R14
	ADCXQ AX, R13
	ADCQ  $0x00, R14

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8
	MOVQ  CX, 8(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 16(SI), DX
	XORQ CX, CX

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9
	MOVQ  R8, 16(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ CX, CX
	ADCXQ BX, CX

	// |
	// |
	MOVQ 24(SI), DX
	XORQ R8, R8

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10
	MOVQ  R9, 24(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, CX
	ADOXQ R8, R8
	ADCXQ BX, R8

	// |
	// |
	MOVQ 32(SI), DX
	XORQ R9, R9

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11
	MOVQ  R10, 32(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R8
	ADOXQ R9, R9
	ADCXQ BX, R9

	// |
	// |
	MOVQ 40(SI), DX
	XORQ R10, R10

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12
	MOVQ  R11, 40(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R9
	ADOXQ R10, R10
	ADCXQ BX, R10

	// |
	// |
	MOVQ 48(SI), DX
	XORQ R11, R11

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R10
	ADOXQ R11, R11
	ADCXQ BX, R11

	// |
	// |
	MOVQ 56(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R11
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ (SP), DI
	MOVQ 8(SP), BX
	MOVQ R15, (SP)
	MOVQ 16(SP), R15
	MOVQ CX, 8(SP)
	MOVQ 24(SP), CX
	MOVQ R8, 16(SP)
	MOVQ 32(SP), R8
	MOVQ R9, 24(SP)
	MOVQ 40(SP), R9
	MOVQ R10, 32(SP)
	MOVQ R11, 40(SP)
	MOVQ SI, 48(SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  R11, R11
	MOVQ  DI, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14
	ADOXQ R11, R14
	ADCXQ R11, R11
	XORQ  DI, DI
	MOVQ  (SP), DI

	// |
	// |
	MOVQ  BX, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI
	ADOXQ R11, DI
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  BX, BX
	MOVQ  8(SP), BX

	// |
	// |
	MOVQ  R15, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX
	ADOXQ R11, BX
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R15, R15
	MOVQ  16(SP), R15

	// |
	// |
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15
	ADOXQ R11, R15
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  CX, CX
	MOVQ  24(SP), CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX
	ADOXQ R11, CX
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R8, R8
	MOVQ  32(SP), R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8
	ADOXQ R11, R8
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R9, R9
	MOVQ  40(SP), R9

	// |
	// |
	MOVQ  R12, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9
	ADOXQ R11, R9
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R12, R12
	MOVQ  48(SP), R12

	// |
	// |
	MOVQ  R13, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12
	ADOXQ R11, R12
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	ADOXQ R13, R11

	// | 

/* reduction 				*/

	MOVQ R14, DX
	SUBQ (SI), DX
	MOVQ DI, R10
	SBBQ 8(SI), R10
	MOVQ BX, R13
	SBBQ 16(SI), R13
	MOVQ R15, AX
	SBBQ 24(SI), AX
	MOVQ AX, (SP)
	MOVQ CX, AX
	SBBQ 32(SI), AX
	MOVQ AX, 8(SP)
	MOVQ R8, AX
	SBBQ 40(SI), AX
	MOVQ AX, 16(SP)
	MOVQ R9, AX
	SBBQ 48(SI), AX
	MOVQ AX, 24(SP)
	MOVQ R12, AX
	SBBQ 56(SI), AX
	MOVQ AX, 32(SP)
	SBBQ $0x00, R11

	// |
	MOVQ    c+0(FP), R11
	CMOVQCC DX, R14
	MOVQ    R14, (R11)
	CMOVQCC R10, DI
	MOVQ    DI, 8(R11)
	CMOVQCC R13, BX
	MOVQ    BX, 16(R11)
	CMOVQCC (SP), R15
	MOVQ    R15, 24(R11)
	CMOVQCC 8(SP), CX
	MOVQ    CX, 32(R11)
	CMOVQCC 16(SP), R8
	MOVQ    R8, 40(R11)
	CMOVQCC 24(SP), R9
	MOVQ    R9, 48(R11)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 56(R11)
	RET

	// | 

/* end 				*/

