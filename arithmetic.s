// Code generated by command: go run gen.go -out ./codegen/generated/arithmetic.s -from 4 -to 8 -globmod=true. DO NOT EDIT.

// func add4(c *Fe256, a *Fe256, b *Fe256)
TEXT ·add4(SB), $0-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ R8, R12
	SUBQ ·modulus4+0(SB), R12
	MOVQ R9, R13
	SBBQ ·modulus4+8(SB), R13
	MOVQ R10, R14
	SBBQ ·modulus4+16(SB), R14
	MOVQ R11, R15
	SBBQ ·modulus4+24(SB), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, R8
	MOVQ    R8, (DI)
	CMOVQCC R13, R9
	MOVQ    R9, 8(DI)
	CMOVQCC R14, R10
	MOVQ    R10, 16(DI)
	CMOVQCC R15, R11
	MOVQ    R11, 24(DI)
	RET

// func addn4(a *Fe256, b *Fe256) uint64
TEXT ·addn4(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func sub4(c *Fe256, a *Fe256, b *Fe256)
TEXT ·sub4(SB), $0-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11

	// |
	MOVQ    ·modulus4+0(SB), R12
	CMOVQCC AX, R12
	MOVQ    ·modulus4+8(SB), R13
	CMOVQCC AX, R13
	MOVQ    ·modulus4+16(SB), R14
	CMOVQCC AX, R14
	MOVQ    ·modulus4+24(SB), R15
	CMOVQCC AX, R15

	// |
	MOVQ c+0(FP), DI
	ADDQ R12, R8
	MOVQ R8, (DI)
	ADCQ R13, R9
	MOVQ R9, 8(DI)
	ADCQ R14, R10
	MOVQ R10, 16(DI)
	ADCQ R15, R11
	MOVQ R11, 24(DI)
	RET

// func subn4(a *Fe256, b *Fe256) uint64
TEXT ·subn4(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double4(c *Fe256, a *Fe256)
TEXT ·double4(SB), $0-16
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), R8
	ADDQ R8, R8
	MOVQ 8(DI), R9
	ADCQ R9, R9
	MOVQ 16(DI), R10
	ADCQ R10, R10
	MOVQ 24(DI), R11
	ADCQ R11, R11
	ADCQ $0x00, AX

	// |
	MOVQ R8, R12
	SUBQ ·modulus4+0(SB), R12
	MOVQ R9, R13
	SBBQ ·modulus4+8(SB), R13
	MOVQ R10, R14
	SBBQ ·modulus4+16(SB), R14
	MOVQ R11, R15
	SBBQ ·modulus4+24(SB), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, R8
	MOVQ    R8, (DI)
	CMOVQCC R13, R9
	MOVQ    R9, 8(DI)
	CMOVQCC R14, R10
	MOVQ    R10, 16(DI)
	CMOVQCC R15, R11
	MOVQ    R11, 24(DI)
	RET

// func neg4(c *Fe256, a *Fe256)
TEXT ·neg4(SB), $0-16
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ ·modulus4+0(SB), R8
	SUBQ (DI), R8
	MOVQ ·modulus4+8(SB), R9
	SBBQ 8(DI), R9
	MOVQ ·modulus4+16(SB), R10
	SBBQ 16(DI), R10
	MOVQ ·modulus4+24(SB), R11
	SBBQ 24(DI), R11

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	RET

// func mul4(c *[8]uint64, a *Fe256, b *Fe256)
TEXT ·mul4(SB), $0-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, (DI)
	MOVQ R13, 8(DI)
	MOVQ R14, 16(DI)
	MOVQ R15, 24(DI)
	RET

// func mont4(c *Fe256, w *[8]uint64)
TEXT ·mont4(SB), $0-16
	MOVQ w+8(FP), DI
	MOVQ (DI), R8
	MOVQ 8(DI), R9
	MOVQ 16(DI), R10
	MOVQ 24(DI), R11
	MOVQ 32(DI), R12

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp4+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ R14, R14
	MOVQ ·modulus4+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14

	// | w1
	XORQ R13, R13
	MOVQ ·modulus4+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w2
	XORQ R14, R14
	MOVQ ·modulus4+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus4+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	ADDQ R13, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 40(DI), R8

	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp4+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ R14, R14
	MOVQ ·modulus4+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14

	// | w2
	XORQ R13, R13
	MOVQ ·modulus4+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R13
	ADDQ R14, R10
	ADCQ $0x00, R13

	// | w3
	XORQ R14, R14
	MOVQ ·modulus4+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14
	ADDQ R13, R11
	ADCQ $0x00, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus4+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	ADDQ R13, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 48(DI), R9

	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp4+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ R14, R14
	MOVQ ·modulus4+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus4+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	XORQ R14, R14
	MOVQ ·modulus4+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus4+24(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R13
	ADDQ R14, R8
	ADCQ $0x00, R13

	// | w6
	ADDQ R13, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(DI), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp4+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ R14, R14
	MOVQ ·modulus4+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus4+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	XORQ R14, R14
	MOVQ ·modulus4+16(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus4+24(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w7
	ADDQ R13, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ R12, R11
	SUBQ ·modulus4+0(SB), R11
	MOVQ R8, R13
	SBBQ ·modulus4+8(SB), R13
	MOVQ R9, R14
	SBBQ ·modulus4+16(SB), R14
	MOVQ R10, CX
	SBBQ ·modulus4+24(SB), CX
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCC R11, R12
	MOVQ    R12, (DI)
	CMOVQCC R13, R8
	MOVQ    R8, 8(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 16(DI)
	CMOVQCC CX, R10
	MOVQ    R10, 24(DI)
	RET

// func montmul4(c *Fe256, a *Fe256, b *Fe256)
TEXT ·montmul4(SB), $8-24
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// |
	// | Montgomerry Reduction
	MOVQ R15, (SP)

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp4+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ DI, DI
	MOVQ ·modulus4+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1
	XORQ SI, SI
	MOVQ ·modulus4+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2
	XORQ DI, DI
	MOVQ ·modulus4+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus4+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	ADDQ SI, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp4+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ DI, DI
	MOVQ ·modulus4+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2
	XORQ SI, SI
	MOVQ ·modulus4+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3
	XORQ DI, DI
	MOVQ ·modulus4+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus4+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	ADDQ SI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp4+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ DI, DI
	MOVQ ·modulus4+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus4+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	XORQ DI, DI
	MOVQ ·modulus4+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus4+24(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	ADDQ SI, R15
	ADCQ R15, R14
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ (SP), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp4+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ DI, DI
	MOVQ ·modulus4+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus4+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	XORQ DI, DI
	MOVQ ·modulus4+16(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus4+24(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	ADDQ SI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ R12, R11
	SUBQ ·modulus4+0(SB), R11
	MOVQ R13, CX
	SBBQ ·modulus4+8(SB), CX
	MOVQ R14, AX
	SBBQ ·modulus4+16(SB), AX
	MOVQ R10, DX
	SBBQ ·modulus4+24(SB), DX
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCC R11, R12
	MOVQ    R12, (DI)
	CMOVQCC CX, R13
	MOVQ    R13, 8(DI)
	CMOVQCC AX, R14
	MOVQ    R14, 16(DI)
	CMOVQCC DX, R10
	MOVQ    R10, 24(DI)
	RET

// func square4(c *[8]uint64, a *Fe256)
TEXT ·square4(SB), $0-16
	RET

// func montsquare4(c *Fe256, a *Fe256)
TEXT ·montsquare4(SB), $0-16
	RET

// func add5(c *Fe320, a *Fe320, b *Fe320)
TEXT ·add5(SB), $0-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	MOVQ 32(DI), R12
	ADCQ 32(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ R8, R13
	SUBQ ·modulus5+0(SB), R13
	MOVQ R9, R14
	SBBQ ·modulus5+8(SB), R14
	MOVQ R10, R15
	SBBQ ·modulus5+16(SB), R15
	MOVQ R11, CX
	SBBQ ·modulus5+24(SB), CX
	MOVQ R12, DX
	SBBQ ·modulus5+32(SB), DX
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, R8
	MOVQ    R8, (DI)
	CMOVQCC R14, R9
	MOVQ    R9, 8(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 16(DI)
	CMOVQCC CX, R11
	MOVQ    R11, 24(DI)
	CMOVQCC DX, R12
	MOVQ    R12, 32(DI)
	RET

// func addn5(a *Fe320, b *Fe320) uint64
TEXT ·addn5(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	MOVQ 32(DI), R12
	ADCQ 32(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func sub5(c *Fe320, a *Fe320, b *Fe320)
TEXT ·sub5(SB), $0-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11
	MOVQ 32(DI), R12
	SBBQ 32(SI), R12

	// |
	MOVQ    ·modulus5+0(SB), R13
	CMOVQCC AX, R13
	MOVQ    ·modulus5+8(SB), R14
	CMOVQCC AX, R14
	MOVQ    ·modulus5+16(SB), R15
	CMOVQCC AX, R15
	MOVQ    ·modulus5+24(SB), CX
	CMOVQCC AX, CX
	MOVQ    ·modulus5+32(SB), DX
	CMOVQCC AX, DX

	// |
	MOVQ c+0(FP), DI
	ADDQ R13, R8
	MOVQ R8, (DI)
	ADCQ R14, R9
	MOVQ R9, 8(DI)
	ADCQ R15, R10
	MOVQ R10, 16(DI)
	ADCQ CX, R11
	MOVQ R11, 24(DI)
	ADCQ DX, R12
	MOVQ R12, 32(DI)
	RET

// func subn5(a *Fe320, b *Fe320) uint64
TEXT ·subn5(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11
	MOVQ 32(DI), R12
	SBBQ 32(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double5(c *Fe320, a *Fe320)
TEXT ·double5(SB), $0-16
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), R8
	ADDQ R8, R8
	MOVQ 8(DI), R9
	ADCQ R9, R9
	MOVQ 16(DI), R10
	ADCQ R10, R10
	MOVQ 24(DI), R11
	ADCQ R11, R11
	MOVQ 32(DI), R12
	ADCQ R12, R12
	ADCQ $0x00, AX

	// |
	MOVQ R8, R13
	SUBQ ·modulus5+0(SB), R13
	MOVQ R9, R14
	SBBQ ·modulus5+8(SB), R14
	MOVQ R10, R15
	SBBQ ·modulus5+16(SB), R15
	MOVQ R11, CX
	SBBQ ·modulus5+24(SB), CX
	MOVQ R12, DX
	SBBQ ·modulus5+32(SB), DX
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, R8
	MOVQ    R8, (DI)
	CMOVQCC R14, R9
	MOVQ    R9, 8(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 16(DI)
	CMOVQCC CX, R11
	MOVQ    R11, 24(DI)
	CMOVQCC DX, R12
	MOVQ    R12, 32(DI)
	RET

// func neg5(c *Fe320, a *Fe320)
TEXT ·neg5(SB), $0-16
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ ·modulus5+0(SB), R8
	SUBQ (DI), R8
	MOVQ ·modulus5+8(SB), R9
	SBBQ 8(DI), R9
	MOVQ ·modulus5+16(SB), R10
	SBBQ 16(DI), R10
	MOVQ ·modulus5+24(SB), R11
	SBBQ 24(DI), R11
	MOVQ ·modulus5+32(SB), R12
	SBBQ 32(DI), R12

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	RET

// func mul5(c *[10]uint64, a *Fe320, b *Fe320)
TEXT ·mul5(SB), $16-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b4
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, (DI)
	MOVQ R14, 8(DI)
	MOVQ R15, 16(DI)
	MOVQ (SP), BX
	MOVQ BX, 24(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 32(DI)
	RET

// func mont5(c *Fe320, w *[10]uint64)
TEXT ·mont5(SB), $0-16
	MOVQ w+8(FP), DI
	MOVQ (DI), R8
	MOVQ 8(DI), R9
	MOVQ 16(DI), R10
	MOVQ 24(DI), R11
	MOVQ 32(DI), R12
	MOVQ 40(DI), SI

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ R14, R14
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14

	// | w1
	XORQ R13, R13
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w2
	XORQ R14, R14
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	XORQ R14, R14
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w5
	ADDQ R14, SI
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 48(DI), R8

	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ R14, R14
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14

	// | w2
	XORQ R13, R13
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R13
	ADDQ R14, R10
	ADCQ $0x00, R13

	// | w3
	XORQ R14, R14
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14
	ADDQ R13, R11
	ADCQ $0x00, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	XORQ R14, R14
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14
	ADDQ R13, SI
	ADCQ $0x00, R14

	// | w6
	ADDQ R14, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(DI), R9

	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ R14, R14
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	XORQ R14, R14
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w7
	ADDQ R14, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(DI), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ R14, R14
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	XORQ R14, R14
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14
	ADDQ R13, SI
	ADCQ $0x00, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R13
	ADDQ R14, R8
	ADCQ $0x00, R13

	// | w7
	XORQ R14, R14
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14
	ADDQ R13, R9
	ADCQ $0x00, R14

	// | w8
	ADDQ R14, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(DI), R11

	// | u @ CX = w4 * inp
	MOVQ R12, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w4
	XORQ R14, R14
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w8
	XORQ R14, R14
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w9
	ADDQ R14, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ SI, R12
	SUBQ ·modulus5+0(SB), R12
	MOVQ R8, R13
	SBBQ ·modulus5+8(SB), R13
	MOVQ R9, R14
	SBBQ ·modulus5+16(SB), R14
	MOVQ R10, CX
	SBBQ ·modulus5+24(SB), CX
	MOVQ R11, AX
	SBBQ ·modulus5+32(SB), AX
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCC R12, SI
	MOVQ    SI, (DI)
	CMOVQCC R13, R8
	MOVQ    R8, 8(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 16(DI)
	CMOVQCC CX, R10
	MOVQ    R10, 24(DI)
	CMOVQCC AX, R11
	MOVQ    R11, 32(DI)
	RET

// func montmul5(c *Fe320, a *Fe320, b *Fe320)
TEXT ·montmul5(SB), $24-24
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b4
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 16(SP)

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ DI, DI
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1
	XORQ SI, SI
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2
	XORQ DI, DI
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	XORQ DI, DI
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5
	ADDQ DI, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ DI, DI
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2
	XORQ SI, SI
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3
	XORQ DI, DI
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	XORQ DI, DI
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6
	ADDQ DI, R15
	ADCQ R15, R14
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 16(SP), R9

	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ DI, DI
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	XORQ DI, DI
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	ADDQ DI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ (SP), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ DI, DI
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	XORQ DI, DI
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	XORQ DI, DI
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w8
	ADDQ DI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 8(SP), R11

	// | u @ CX = w4 * inp
	MOVQ R12, AX
	MULQ ·inp5+0(SB)
	MOVQ AX, CX

	// | w4
	XORQ DI, DI
	MOVQ ·modulus5+0(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus5+8(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus5+16(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus5+24(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w8
	XORQ DI, DI
	MOVQ ·modulus5+32(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w9
	ADDQ DI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ R13, R12
	SUBQ ·modulus5+0(SB), R12
	MOVQ R14, CX
	SBBQ ·modulus5+8(SB), CX
	MOVQ R9, AX
	SBBQ ·modulus5+16(SB), AX
	MOVQ R10, DX
	SBBQ ·modulus5+24(SB), DX
	MOVQ R11, SI
	SBBQ ·modulus5+32(SB), SI
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCC R12, R13
	MOVQ    R13, (DI)
	CMOVQCC CX, R14
	MOVQ    R14, 8(DI)
	CMOVQCC AX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC DX, R10
	MOVQ    R10, 24(DI)
	CMOVQCC SI, R11
	MOVQ    R11, 32(DI)
	RET

// func square5(c *[10]uint64, a *Fe320)
TEXT ·square5(SB), $0-16
	RET

// func montsquare5(c *Fe320, a *Fe320)
TEXT ·montsquare5(SB), $0-16
	RET

// func add6(c *Fe384, a *Fe384, b *Fe384)
TEXT ·add6(SB), $8-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	MOVQ 32(DI), R12
	ADCQ 32(SI), R12
	MOVQ 40(DI), R13
	ADCQ 40(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ R8, R14
	SUBQ ·modulus6+0(SB), R14
	MOVQ R9, R15
	SBBQ ·modulus6+8(SB), R15
	MOVQ R10, CX
	SBBQ ·modulus6+16(SB), CX
	MOVQ R11, DX
	SBBQ ·modulus6+24(SB), DX
	MOVQ R12, SI
	SBBQ ·modulus6+32(SB), SI
	MOVQ R13, BX
	SBBQ ·modulus6+40(SB), BX
	MOVQ BX, (SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, R8
	MOVQ    R8, (DI)
	CMOVQCC R15, R9
	MOVQ    R9, 8(DI)
	CMOVQCC CX, R10
	MOVQ    R10, 16(DI)
	CMOVQCC DX, R11
	MOVQ    R11, 24(DI)
	CMOVQCC SI, R12
	MOVQ    R12, 32(DI)
	CMOVQCC (SP), R13
	MOVQ    R13, 40(DI)
	RET

// func addn6(a *Fe384, b *Fe384) uint64
TEXT ·addn6(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	MOVQ 32(DI), R12
	ADCQ 32(SI), R12
	MOVQ 40(DI), R13
	ADCQ 40(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func sub6(c *Fe384, a *Fe384, b *Fe384)
TEXT ·sub6(SB), $8-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11
	MOVQ 32(DI), R12
	SBBQ 32(SI), R12
	MOVQ 40(DI), R13
	SBBQ 40(SI), R13

	// |
	MOVQ    ·modulus6+0(SB), R14
	CMOVQCC AX, R14
	MOVQ    ·modulus6+8(SB), R15
	CMOVQCC AX, R15
	MOVQ    ·modulus6+16(SB), CX
	CMOVQCC AX, CX
	MOVQ    ·modulus6+24(SB), DX
	CMOVQCC AX, DX
	MOVQ    ·modulus6+32(SB), SI
	CMOVQCC AX, SI
	CMOVQCS ·modulus6+40(SB), AX
	MOVQ    AX, (SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R14, R8
	MOVQ R8, (DI)
	ADCQ R15, R9
	MOVQ R9, 8(DI)
	ADCQ CX, R10
	MOVQ R10, 16(DI)
	ADCQ DX, R11
	MOVQ R11, 24(DI)
	ADCQ SI, R12
	MOVQ R12, 32(DI)
	ADCQ (SP), R13
	MOVQ R13, 40(DI)
	RET

// func subn6(a *Fe384, b *Fe384) uint64
TEXT ·subn6(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11
	MOVQ 32(DI), R12
	SBBQ 32(SI), R12
	MOVQ 40(DI), R13
	SBBQ 40(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double6(c *Fe384, a *Fe384)
TEXT ·double6(SB), $8-16
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), R8
	ADDQ R8, R8
	MOVQ 8(DI), R9
	ADCQ R9, R9
	MOVQ 16(DI), R10
	ADCQ R10, R10
	MOVQ 24(DI), R11
	ADCQ R11, R11
	MOVQ 32(DI), R12
	ADCQ R12, R12
	MOVQ 40(DI), R13
	ADCQ R13, R13
	ADCQ $0x00, AX

	// |
	MOVQ R8, R14
	SUBQ ·modulus6+0(SB), R14
	MOVQ R9, R15
	SBBQ ·modulus6+8(SB), R15
	MOVQ R10, CX
	SBBQ ·modulus6+16(SB), CX
	MOVQ R11, DX
	SBBQ ·modulus6+24(SB), DX
	MOVQ R12, SI
	SBBQ ·modulus6+32(SB), SI
	MOVQ R13, BX
	SBBQ ·modulus6+40(SB), BX
	MOVQ BX, (SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, R8
	MOVQ    R8, (DI)
	CMOVQCC R15, R9
	MOVQ    R9, 8(DI)
	CMOVQCC CX, R10
	MOVQ    R10, 16(DI)
	CMOVQCC DX, R11
	MOVQ    R11, 24(DI)
	CMOVQCC SI, R12
	MOVQ    R12, 32(DI)
	CMOVQCC (SP), R13
	MOVQ    R13, 40(DI)
	RET

// func neg6(c *Fe384, a *Fe384)
TEXT ·neg6(SB), $0-16
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ ·modulus6+0(SB), R8
	SUBQ (DI), R8
	MOVQ ·modulus6+8(SB), R9
	SBBQ 8(DI), R9
	MOVQ ·modulus6+16(SB), R10
	SBBQ 16(DI), R10
	MOVQ ·modulus6+24(SB), R11
	SBBQ 24(DI), R11
	MOVQ ·modulus6+32(SB), R12
	SBBQ 32(DI), R12
	MOVQ ·modulus6+40(SB), R13
	SBBQ 40(DI), R13

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	RET

// func mul6(c *[12]uint64, a *Fe384, b *Fe384)
TEXT ·mul6(SB), $32-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b5
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, (DI)
	MOVQ R15, 8(DI)
	MOVQ (SP), BX
	MOVQ BX, 16(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 24(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 32(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 40(DI)
	RET

// func mont6(c *Fe384, w *[12]uint64)
TEXT ·mont6(SB), $8-16
	MOVQ w+8(FP), DI
	MOVQ (DI), R8
	MOVQ 8(DI), R9
	MOVQ 16(DI), R10
	MOVQ 24(DI), R11
	MOVQ 32(DI), R12
	MOVQ 40(DI), SI
	MOVQ 48(DI), BX
	MOVQ BX, (SP)

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ R14, R14
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14

	// | w1
	XORQ R13, R13
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w2
	XORQ R14, R14
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	XORQ R14, R14
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	ADDQ R13, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(DI), R8

	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ R14, R14
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14

	// | w2
	XORQ R13, R13
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R13
	ADDQ R14, R10
	ADCQ $0x00, R13

	// | w3
	XORQ R14, R14
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14
	ADDQ R13, R11
	ADCQ $0x00, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	XORQ R14, R14
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14
	ADDQ R13, SI
	ADCQ $0x00, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R13
	ADDQ R14, (SP)
	ADCQ $0x00, R13

	// | w7
	ADDQ R13, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(DI), R9

	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ R14, R14
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	XORQ R14, R14
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14
	ADDQ R13, (SP)
	ADCQ $0x00, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R13
	ADDQ R14, R8
	ADCQ $0x00, R13

	// | w8
	ADDQ R13, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(DI), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ R14, R14
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	XORQ R14, R14
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14
	ADDQ R13, SI
	ADCQ $0x00, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R13
	ADDQ R14, (SP)
	ADCQ $0x00, R13

	// | w7
	XORQ R14, R14
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w8
	XORQ R13, R13
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w9
	ADDQ R13, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(DI), R11

	// | u @ CX = w4 * inp
	MOVQ R12, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w4
	XORQ R14, R14
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14
	ADDQ R13, (SP)
	ADCQ $0x00, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R13
	ADDQ R14, R8
	ADCQ $0x00, R13

	// | w8
	XORQ R14, R14
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14
	ADDQ R13, R9
	ADCQ $0x00, R14

	// | w9
	XORQ R13, R13
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R13
	ADDQ R14, R10
	ADCQ $0x00, R13

	// | w10
	ADDQ R13, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(DI), R12

	// | u @ CX = w5 * inp
	MOVQ SI, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w5
	XORQ R14, R14
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R13
	ADDQ R14, (SP)
	ADCQ $0x00, R13

	// | w7
	XORQ R14, R14
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w8
	XORQ R13, R13
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w9
	XORQ R14, R14
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w10
	XORQ R13, R13
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w11
	ADDQ R13, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ (SP), R13
	SUBQ ·modulus6+0(SB), R13
	MOVQ R8, R14
	SBBQ ·modulus6+8(SB), R14
	MOVQ R9, CX
	SBBQ ·modulus6+16(SB), CX
	MOVQ R10, AX
	SBBQ ·modulus6+24(SB), AX
	MOVQ R11, DX
	SBBQ ·modulus6+32(SB), DX
	MOVQ R12, SI
	SBBQ ·modulus6+40(SB), SI
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	MOVQ    (SP), BX
	CMOVQCC R13, BX
	MOVQ    BX, (SP)
	MOVQ    (SP), BX
	MOVQ    BX, (DI)
	CMOVQCC R14, R8
	MOVQ    R8, 8(DI)
	CMOVQCC CX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC AX, R10
	MOVQ    R10, 24(DI)
	CMOVQCC DX, R11
	MOVQ    R11, 32(DI)
	CMOVQCC SI, R12
	MOVQ    R12, 40(DI)
	RET

// func montmul6(c *Fe384, a *Fe384, b *Fe384)
TEXT ·montmul6(SB), $48-24
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b5
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 32(SP)

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ DI, DI
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1
	XORQ SI, SI
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2
	XORQ DI, DI
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	XORQ DI, DI
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	ADDQ SI, R14
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 32(SP), R8

	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ DI, DI
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2
	XORQ SI, SI
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3
	XORQ DI, DI
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	XORQ DI, DI
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	ADDQ SI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ (SP), R9

	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ DI, DI
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	XORQ DI, DI
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w8
	ADDQ SI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 8(SP), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ DI, DI
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	XORQ DI, DI
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	XORQ DI, DI
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w8
	XORQ SI, SI
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w9
	ADDQ SI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 16(SP), R11

	// | u @ CX = w4 * inp
	MOVQ R12, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w4
	XORQ DI, DI
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w8
	XORQ DI, DI
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w9
	XORQ SI, SI
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w10
	ADDQ SI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 24(SP), R12

	// | u @ CX = w5 * inp
	MOVQ R13, AX
	MULQ ·inp6+0(SB)
	MOVQ AX, CX

	// | w5
	XORQ DI, DI
	MOVQ ·modulus6+0(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus6+8(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	XORQ DI, DI
	MOVQ ·modulus6+16(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w8
	XORQ SI, SI
	MOVQ ·modulus6+24(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w9
	XORQ DI, DI
	MOVQ ·modulus6+32(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w10
	XORQ SI, SI
	MOVQ ·modulus6+40(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w11
	ADDQ SI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ R14, R13
	SUBQ ·modulus6+0(SB), R13
	MOVQ R8, CX
	SBBQ ·modulus6+8(SB), CX
	MOVQ R9, AX
	SBBQ ·modulus6+16(SB), AX
	MOVQ R10, DX
	SBBQ ·modulus6+24(SB), DX
	MOVQ R11, SI
	SBBQ ·modulus6+32(SB), SI
	MOVQ R12, BX
	SBBQ ·modulus6+40(SB), BX
	MOVQ BX, 40(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCC R13, R14
	MOVQ    R14, (DI)
	CMOVQCC CX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC AX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC DX, R10
	MOVQ    R10, 24(DI)
	CMOVQCC SI, R11
	MOVQ    R11, 32(DI)
	CMOVQCC 40(SP), R12
	MOVQ    R12, 40(DI)
	RET

// func square6(c *[12]uint64, a *Fe384)
TEXT ·square6(SB), $0-16
	RET

// func montsquare6(c *Fe384, a *Fe384)
TEXT ·montsquare6(SB), $0-16
	RET

// func add7(c *Fe448, a *Fe448, b *Fe448)
TEXT ·add7(SB), $24-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	MOVQ 32(DI), R12
	ADCQ 32(SI), R12
	MOVQ 40(DI), R13
	ADCQ 40(SI), R13
	MOVQ 48(DI), R14
	ADCQ 48(SI), R14
	ADCQ $0x00, AX

	// |
	MOVQ R8, R15
	SUBQ ·modulus7+0(SB), R15
	MOVQ R9, CX
	SBBQ ·modulus7+8(SB), CX
	MOVQ R10, DX
	SBBQ ·modulus7+16(SB), DX
	MOVQ R11, SI
	SBBQ ·modulus7+24(SB), SI
	MOVQ R12, BX
	SBBQ ·modulus7+32(SB), BX
	MOVQ BX, (SP)
	MOVQ R13, BX
	SBBQ ·modulus7+40(SB), BX
	MOVQ BX, 8(SP)
	MOVQ R14, BX
	SBBQ ·modulus7+48(SB), BX
	MOVQ BX, 16(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R15, R8
	MOVQ    R8, (DI)
	CMOVQCC CX, R9
	MOVQ    R9, 8(DI)
	CMOVQCC DX, R10
	MOVQ    R10, 16(DI)
	CMOVQCC SI, R11
	MOVQ    R11, 24(DI)
	CMOVQCC (SP), R12
	MOVQ    R12, 32(DI)
	CMOVQCC 8(SP), R13
	MOVQ    R13, 40(DI)
	CMOVQCC 16(SP), R14
	MOVQ    R14, 48(DI)
	RET

// func addn7(a *Fe448, b *Fe448) uint64
TEXT ·addn7(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	MOVQ 32(DI), R12
	ADCQ 32(SI), R12
	MOVQ 40(DI), R13
	ADCQ 40(SI), R13
	MOVQ 48(DI), R14
	ADCQ 48(SI), R14
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func sub7(c *Fe448, a *Fe448, b *Fe448)
TEXT ·sub7(SB), $24-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11
	MOVQ 32(DI), R12
	SBBQ 32(SI), R12
	MOVQ 40(DI), R13
	SBBQ 40(SI), R13
	MOVQ 48(DI), R14
	SBBQ 48(SI), R14

	// |
	MOVQ    ·modulus7+0(SB), R15
	CMOVQCC AX, R15
	MOVQ    ·modulus7+8(SB), CX
	CMOVQCC AX, CX
	MOVQ    ·modulus7+16(SB), DX
	CMOVQCC AX, DX
	MOVQ    ·modulus7+24(SB), SI
	CMOVQCC AX, SI
	CMOVQCS ·modulus7+32(SB), AX
	MOVQ    AX, (SP)
	CMOVQCS ·modulus7+40(SB), AX
	MOVQ    AX, 8(SP)
	CMOVQCS ·modulus7+48(SB), AX
	MOVQ    AX, 16(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R15, R8
	MOVQ R8, (DI)
	ADCQ CX, R9
	MOVQ R9, 8(DI)
	ADCQ DX, R10
	MOVQ R10, 16(DI)
	ADCQ SI, R11
	MOVQ R11, 24(DI)
	ADCQ (SP), R12
	MOVQ R12, 32(DI)
	ADCQ 8(SP), R13
	MOVQ R13, 40(DI)
	ADCQ 16(SP), R14
	MOVQ R14, 48(DI)
	RET

// func subn7(a *Fe448, b *Fe448) uint64
TEXT ·subn7(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11
	MOVQ 32(DI), R12
	SBBQ 32(SI), R12
	MOVQ 40(DI), R13
	SBBQ 40(SI), R13
	MOVQ 48(DI), R14
	SBBQ 48(SI), R14
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double7(c *Fe448, a *Fe448)
TEXT ·double7(SB), $24-16
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), R8
	ADDQ R8, R8
	MOVQ 8(DI), R9
	ADCQ R9, R9
	MOVQ 16(DI), R10
	ADCQ R10, R10
	MOVQ 24(DI), R11
	ADCQ R11, R11
	MOVQ 32(DI), R12
	ADCQ R12, R12
	MOVQ 40(DI), R13
	ADCQ R13, R13
	MOVQ 48(DI), R14
	ADCQ R14, R14
	ADCQ $0x00, AX

	// |
	MOVQ R8, R15
	SUBQ ·modulus7+0(SB), R15
	MOVQ R9, CX
	SBBQ ·modulus7+8(SB), CX
	MOVQ R10, DX
	SBBQ ·modulus7+16(SB), DX
	MOVQ R11, SI
	SBBQ ·modulus7+24(SB), SI
	MOVQ R12, BX
	SBBQ ·modulus7+32(SB), BX
	MOVQ BX, (SP)
	MOVQ R13, BX
	SBBQ ·modulus7+40(SB), BX
	MOVQ BX, 8(SP)
	MOVQ R14, BX
	SBBQ ·modulus7+48(SB), BX
	MOVQ BX, 16(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R15, R8
	MOVQ    R8, (DI)
	CMOVQCC CX, R9
	MOVQ    R9, 8(DI)
	CMOVQCC DX, R10
	MOVQ    R10, 16(DI)
	CMOVQCC SI, R11
	MOVQ    R11, 24(DI)
	CMOVQCC (SP), R12
	MOVQ    R12, 32(DI)
	CMOVQCC 8(SP), R13
	MOVQ    R13, 40(DI)
	CMOVQCC 16(SP), R14
	MOVQ    R14, 48(DI)
	RET

// func neg7(c *Fe448, a *Fe448)
TEXT ·neg7(SB), $0-16
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ ·modulus7+0(SB), R8
	SUBQ (DI), R8
	MOVQ ·modulus7+8(SB), R9
	SBBQ 8(DI), R9
	MOVQ ·modulus7+16(SB), R10
	SBBQ 16(DI), R10
	MOVQ ·modulus7+24(SB), R11
	SBBQ 24(DI), R11
	MOVQ ·modulus7+32(SB), R12
	SBBQ 32(DI), R12
	MOVQ ·modulus7+40(SB), R13
	SBBQ 40(DI), R13
	MOVQ ·modulus7+48(SB), R14
	SBBQ 48(DI), R14

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	RET

// func mul7(c *[14]uint64, a *Fe448, b *Fe448)
TEXT ·mul7(SB), $48-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b6
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, (DI)
	MOVQ (SP), BX
	MOVQ BX, 8(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 16(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 24(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 32(DI)
	MOVQ 32(SP), BX
	MOVQ BX, 40(DI)
	MOVQ 40(SP), BX
	MOVQ BX, 48(DI)
	RET

// func mont7(c *Fe448, w *[14]uint64)
TEXT ·mont7(SB), $24-16
	MOVQ w+8(FP), DI
	MOVQ (DI), R8
	MOVQ 8(DI), R9
	MOVQ 16(DI), R10
	MOVQ 24(DI), R11
	MOVQ 32(DI), R12
	MOVQ 40(DI), SI
	MOVQ 48(DI), BX
	MOVQ BX, (SP)
	MOVQ 56(DI), BX
	MOVQ BX, 8(SP)

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ R14, R14
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14

	// | w1
	XORQ R13, R13
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w2
	XORQ R14, R14
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	XORQ R14, R14
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14
	ADDQ R13, (SP)
	ADCQ $0x00, R14

	// | w7
	ADDQ R14, 8(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(DI), R8

	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ R14, R14
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14

	// | w2
	XORQ R13, R13
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R13
	ADDQ R14, R10
	ADCQ $0x00, R13

	// | w3
	XORQ R14, R14
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14
	ADDQ R13, R11
	ADCQ $0x00, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	XORQ R14, R14
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14
	ADDQ R13, SI
	ADCQ $0x00, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R13
	ADDQ R14, (SP)
	ADCQ $0x00, R13

	// | w7
	XORQ R14, R14
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R14
	ADDQ R13, 8(SP)
	ADCQ $0x00, R14

	// | w8
	ADDQ R14, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(DI), R9

	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ R14, R14
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	XORQ R14, R14
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14
	ADDQ R13, (SP)
	ADCQ $0x00, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R13
	ADDQ R14, 8(SP)
	ADCQ $0x00, R13

	// | w8
	XORQ R14, R14
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w9
	ADDQ R14, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(DI), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ R14, R14
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	XORQ R14, R14
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14
	ADDQ R13, SI
	ADCQ $0x00, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R13
	ADDQ R14, (SP)
	ADCQ $0x00, R13

	// | w7
	XORQ R14, R14
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R14
	ADDQ R13, 8(SP)
	ADCQ $0x00, R14

	// | w8
	XORQ R13, R13
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R13
	ADDQ R14, R8
	ADCQ $0x00, R13

	// | w9
	XORQ R14, R14
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14
	ADDQ R13, R9
	ADCQ $0x00, R14

	// | w10
	ADDQ R14, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(DI), R11

	// | u @ CX = w4 * inp
	MOVQ R12, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w4
	XORQ R14, R14
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14
	ADDQ R13, (SP)
	ADCQ $0x00, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R13
	ADDQ R14, 8(SP)
	ADCQ $0x00, R13

	// | w8
	XORQ R14, R14
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w9
	XORQ R13, R13
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w10
	XORQ R14, R14
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w11
	ADDQ R14, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 96(DI), R12

	// | u @ CX = w5 * inp
	MOVQ SI, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w5
	XORQ R14, R14
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R13
	ADDQ R14, (SP)
	ADCQ $0x00, R13

	// | w7
	XORQ R14, R14
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R14
	ADDQ R13, 8(SP)
	ADCQ $0x00, R14

	// | w8
	XORQ R13, R13
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R13
	ADDQ R14, R8
	ADCQ $0x00, R13

	// | w9
	XORQ R14, R14
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14
	ADDQ R13, R9
	ADCQ $0x00, R14

	// | w10
	XORQ R13, R13
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R13
	ADDQ R14, R10
	ADCQ $0x00, R13

	// | w11
	XORQ R14, R14
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14
	ADDQ R13, R11
	ADCQ $0x00, R14

	// | w12
	ADDQ R14, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 104(DI), SI

	// | u @ CX = w6 * inp
	MOVQ (SP), AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w6
	XORQ R14, R14
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R13
	ADDQ R14, 8(SP)
	ADCQ $0x00, R13

	// | w8
	XORQ R14, R14
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w9
	XORQ R13, R13
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w10
	XORQ R14, R14
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w11
	XORQ R13, R13
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w12
	XORQ R14, R14
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w13
	ADDQ R14, R15
	ADCQ R15, SI
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 8(SP), R13
	SUBQ ·modulus7+0(SB), R13
	MOVQ R8, R14
	SBBQ ·modulus7+8(SB), R14
	MOVQ R9, CX
	SBBQ ·modulus7+16(SB), CX
	MOVQ R10, AX
	SBBQ ·modulus7+24(SB), AX
	MOVQ R11, DX
	SBBQ ·modulus7+32(SB), DX
	MOVQ R12, BX
	SBBQ ·modulus7+40(SB), BX
	MOVQ BX, (SP)
	MOVQ SI, BX
	SBBQ ·modulus7+48(SB), BX
	MOVQ BX, 16(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	MOVQ    8(SP), BX
	CMOVQCC R13, BX
	MOVQ    BX, 8(SP)
	MOVQ    8(SP), BX
	MOVQ    BX, (DI)
	CMOVQCC R14, R8
	MOVQ    R8, 8(DI)
	CMOVQCC CX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC AX, R10
	MOVQ    R10, 24(DI)
	CMOVQCC DX, R11
	MOVQ    R11, 32(DI)
	CMOVQCC (SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 16(SP), SI
	MOVQ    SI, 48(DI)
	RET

// func montmul7(c *Fe448, a *Fe448, b *Fe448)
TEXT ·montmul7(SB), $72-24
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b6
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 48(SP)

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ DI, DI
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1
	XORQ SI, SI
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2
	XORQ DI, DI
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	XORQ DI, DI
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	ADDQ DI, 48(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ (SP), R8

	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ DI, DI
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2
	XORQ SI, SI
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3
	XORQ DI, DI
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	XORQ DI, DI
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	XORQ DI, DI
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w8
	ADDQ DI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 8(SP), R9

	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ DI, DI
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	XORQ DI, DI
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w8
	XORQ DI, DI
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w9
	ADDQ DI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 16(SP), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ DI, DI
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	XORQ DI, DI
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	XORQ DI, DI
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w8
	XORQ SI, SI
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w9
	XORQ DI, DI
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w10
	ADDQ DI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 24(SP), R11

	// | u @ CX = w4 * inp
	MOVQ R12, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w4
	XORQ DI, DI
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w8
	XORQ DI, DI
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w9
	XORQ SI, SI
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w10
	XORQ DI, DI
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w11
	ADDQ DI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 32(SP), R12

	// | u @ CX = w5 * inp
	MOVQ R13, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w5
	XORQ DI, DI
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	XORQ DI, DI
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w8
	XORQ SI, SI
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w9
	XORQ DI, DI
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w10
	XORQ SI, SI
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w11
	XORQ DI, DI
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w12
	ADDQ DI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 40(SP), R13

	// | u @ CX = w6 * inp
	MOVQ R14, AX
	MULQ ·inp7+0(SB)
	MOVQ AX, CX

	// | w6
	XORQ DI, DI
	MOVQ ·modulus7+0(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus7+8(SB), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w8
	XORQ DI, DI
	MOVQ ·modulus7+16(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w9
	XORQ SI, SI
	MOVQ ·modulus7+24(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w10
	XORQ DI, DI
	MOVQ ·modulus7+32(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w11
	XORQ SI, SI
	MOVQ ·modulus7+40(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w12
	XORQ DI, DI
	MOVQ ·modulus7+48(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w13
	ADDQ DI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 48(SP), R14
	SUBQ ·modulus7+0(SB), R14
	MOVQ R8, CX
	SBBQ ·modulus7+8(SB), CX
	MOVQ R9, AX
	SBBQ ·modulus7+16(SB), AX
	MOVQ R10, DX
	SBBQ ·modulus7+24(SB), DX
	MOVQ R11, SI
	SBBQ ·modulus7+32(SB), SI
	MOVQ R12, BX
	SBBQ ·modulus7+40(SB), BX
	MOVQ BX, 56(SP)
	MOVQ R13, BX
	SBBQ ·modulus7+48(SB), BX
	MOVQ BX, 64(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	MOVQ    48(SP), BX
	CMOVQCC R14, BX
	MOVQ    BX, 48(SP)
	MOVQ    48(SP), BX
	MOVQ    BX, (DI)
	CMOVQCC CX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC AX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC DX, R10
	MOVQ    R10, 24(DI)
	CMOVQCC SI, R11
	MOVQ    R11, 32(DI)
	CMOVQCC 56(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 64(SP), R13
	MOVQ    R13, 48(DI)
	RET

// func square7(c *[14]uint64, a *Fe448)
TEXT ·square7(SB), $0-16
	RET

// func montsquare7(c *Fe448, a *Fe448)
TEXT ·montsquare7(SB), $0-16
	RET

// func add8(c *Fe512, a *Fe512, b *Fe512)
TEXT ·add8(SB), $40-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	MOVQ 32(DI), R12
	ADCQ 32(SI), R12
	MOVQ 40(DI), R13
	ADCQ 40(SI), R13
	MOVQ 48(DI), R14
	ADCQ 48(SI), R14
	MOVQ 56(DI), R15
	ADCQ 56(SI), R15
	ADCQ $0x00, AX

	// |
	MOVQ R8, CX
	SUBQ ·modulus8+0(SB), CX
	MOVQ R9, DX
	SBBQ ·modulus8+8(SB), DX
	MOVQ R10, SI
	SBBQ ·modulus8+16(SB), SI
	MOVQ R11, BX
	SBBQ ·modulus8+24(SB), BX
	MOVQ BX, (SP)
	MOVQ R12, BX
	SBBQ ·modulus8+32(SB), BX
	MOVQ BX, 8(SP)
	MOVQ R13, BX
	SBBQ ·modulus8+40(SB), BX
	MOVQ BX, 16(SP)
	MOVQ R14, BX
	SBBQ ·modulus8+48(SB), BX
	MOVQ BX, 24(SP)
	MOVQ R15, BX
	SBBQ ·modulus8+56(SB), BX
	MOVQ BX, 32(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC CX, R8
	MOVQ    R8, (DI)
	CMOVQCC DX, R9
	MOVQ    R9, 8(DI)
	CMOVQCC SI, R10
	MOVQ    R10, 16(DI)
	CMOVQCC (SP), R11
	MOVQ    R11, 24(DI)
	CMOVQCC 8(SP), R12
	MOVQ    R12, 32(DI)
	CMOVQCC 16(SP), R13
	MOVQ    R13, 40(DI)
	CMOVQCC 24(SP), R14
	MOVQ    R14, 48(DI)
	CMOVQCC 32(SP), R15
	MOVQ    R15, 56(DI)
	RET

// func addn8(a *Fe512, b *Fe512) uint64
TEXT ·addn8(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	ADDQ (SI), R8
	MOVQ 8(DI), R9
	ADCQ 8(SI), R9
	MOVQ 16(DI), R10
	ADCQ 16(SI), R10
	MOVQ 24(DI), R11
	ADCQ 24(SI), R11
	MOVQ 32(DI), R12
	ADCQ 32(SI), R12
	MOVQ 40(DI), R13
	ADCQ 40(SI), R13
	MOVQ 48(DI), R14
	ADCQ 48(SI), R14
	MOVQ 56(DI), R15
	ADCQ 56(SI), R15
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func sub8(c *Fe512, a *Fe512, b *Fe512)
TEXT ·sub8(SB), $40-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11
	MOVQ 32(DI), R12
	SBBQ 32(SI), R12
	MOVQ 40(DI), R13
	SBBQ 40(SI), R13
	MOVQ 48(DI), R14
	SBBQ 48(SI), R14
	MOVQ 56(DI), R15
	SBBQ 56(SI), R15

	// |
	MOVQ    ·modulus8+0(SB), CX
	CMOVQCC AX, CX
	MOVQ    ·modulus8+8(SB), DX
	CMOVQCC AX, DX
	MOVQ    ·modulus8+16(SB), SI
	CMOVQCC AX, SI
	CMOVQCS ·modulus8+24(SB), AX
	MOVQ    AX, (SP)
	CMOVQCS ·modulus8+32(SB), AX
	MOVQ    AX, 8(SP)
	CMOVQCS ·modulus8+40(SB), AX
	MOVQ    AX, 16(SP)
	CMOVQCS ·modulus8+48(SB), AX
	MOVQ    AX, 24(SP)
	CMOVQCS ·modulus8+56(SB), AX
	MOVQ    AX, 32(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ CX, R8
	MOVQ R8, (DI)
	ADCQ DX, R9
	MOVQ R9, 8(DI)
	ADCQ SI, R10
	MOVQ R10, 16(DI)
	ADCQ (SP), R11
	MOVQ R11, 24(DI)
	ADCQ 8(SP), R12
	MOVQ R12, 32(DI)
	ADCQ 16(SP), R13
	MOVQ R13, 40(DI)
	ADCQ 24(SP), R14
	MOVQ R14, 48(DI)
	ADCQ 32(SP), R15
	MOVQ R15, 56(DI)
	RET

// func subn8(a *Fe512, b *Fe512) uint64
TEXT ·subn8(SB), $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), R8
	SUBQ (SI), R8
	MOVQ 8(DI), R9
	SBBQ 8(SI), R9
	MOVQ 16(DI), R10
	SBBQ 16(SI), R10
	MOVQ 24(DI), R11
	SBBQ 24(SI), R11
	MOVQ 32(DI), R12
	SBBQ 32(SI), R12
	MOVQ 40(DI), R13
	SBBQ 40(SI), R13
	MOVQ 48(DI), R14
	SBBQ 48(SI), R14
	MOVQ 56(DI), R15
	SBBQ 56(SI), R15
	ADCQ $0x00, AX

	// |
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double8(c *Fe512, a *Fe512)
TEXT ·double8(SB), $40-16
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), R8
	ADDQ R8, R8
	MOVQ 8(DI), R9
	ADCQ R9, R9
	MOVQ 16(DI), R10
	ADCQ R10, R10
	MOVQ 24(DI), R11
	ADCQ R11, R11
	MOVQ 32(DI), R12
	ADCQ R12, R12
	MOVQ 40(DI), R13
	ADCQ R13, R13
	MOVQ 48(DI), R14
	ADCQ R14, R14
	MOVQ 56(DI), R15
	ADCQ R15, R15
	ADCQ $0x00, AX

	// |
	MOVQ R8, CX
	SUBQ ·modulus8+0(SB), CX
	MOVQ R9, DX
	SBBQ ·modulus8+8(SB), DX
	MOVQ R10, SI
	SBBQ ·modulus8+16(SB), SI
	MOVQ R11, BX
	SBBQ ·modulus8+24(SB), BX
	MOVQ BX, (SP)
	MOVQ R12, BX
	SBBQ ·modulus8+32(SB), BX
	MOVQ BX, 8(SP)
	MOVQ R13, BX
	SBBQ ·modulus8+40(SB), BX
	MOVQ BX, 16(SP)
	MOVQ R14, BX
	SBBQ ·modulus8+48(SB), BX
	MOVQ BX, 24(SP)
	MOVQ R15, BX
	SBBQ ·modulus8+56(SB), BX
	MOVQ BX, 32(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC CX, R8
	MOVQ    R8, (DI)
	CMOVQCC DX, R9
	MOVQ    R9, 8(DI)
	CMOVQCC SI, R10
	MOVQ    R10, 16(DI)
	CMOVQCC (SP), R11
	MOVQ    R11, 24(DI)
	CMOVQCC 8(SP), R12
	MOVQ    R12, 32(DI)
	CMOVQCC 16(SP), R13
	MOVQ    R13, 40(DI)
	CMOVQCC 24(SP), R14
	MOVQ    R14, 48(DI)
	CMOVQCC 32(SP), R15
	MOVQ    R15, 56(DI)
	RET

// func neg8(c *Fe512, a *Fe512)
TEXT ·neg8(SB), $0-16
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ ·modulus8+0(SB), R8
	SUBQ (DI), R8
	MOVQ ·modulus8+8(SB), R9
	SBBQ 8(DI), R9
	MOVQ ·modulus8+16(SB), R10
	SBBQ 16(DI), R10
	MOVQ ·modulus8+24(SB), R11
	SBBQ 24(DI), R11
	MOVQ ·modulus8+32(SB), R12
	SBBQ 32(DI), R12
	MOVQ ·modulus8+40(SB), R13
	SBBQ 40(DI), R13
	MOVQ ·modulus8+48(SB), R14
	SBBQ 48(DI), R14
	MOVQ ·modulus8+56(SB), R15
	SBBQ 56(DI), R15

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, 56(DI)
	RET

// func mul8(c *[16]uint64, a *Fe512, b *Fe512)
TEXT ·mul8(SB), $64-24
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b7
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ R8, (DI)
	MOVQ R9, 8(DI)
	MOVQ R10, 16(DI)
	MOVQ R11, 24(DI)
	MOVQ R12, 32(DI)
	MOVQ R13, 40(DI)
	MOVQ R14, 48(DI)
	MOVQ R15, 56(DI)
	MOVQ (SP), BX
	MOVQ BX, (DI)
	MOVQ 8(SP), BX
	MOVQ BX, 8(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 16(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 24(DI)
	MOVQ 32(SP), BX
	MOVQ BX, 32(DI)
	MOVQ 40(SP), BX
	MOVQ BX, 40(DI)
	MOVQ 48(SP), BX
	MOVQ BX, 48(DI)
	MOVQ 56(SP), BX
	MOVQ BX, 56(DI)
	RET

// func mont8(c *Fe512, w *[16]uint64)
TEXT ·mont8(SB), $40-16
	MOVQ w+8(FP), DI
	MOVQ (DI), R8
	MOVQ 8(DI), R9
	MOVQ 16(DI), R10
	MOVQ 24(DI), R11
	MOVQ 32(DI), R12
	MOVQ 40(DI), SI
	MOVQ 48(DI), BX
	MOVQ BX, (SP)
	MOVQ 56(DI), BX
	MOVQ BX, 8(SP)
	MOVQ 64(DI), BX
	MOVQ BX, 16(SP)

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ R14, R14
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14

	// | w1
	XORQ R13, R13
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w2
	XORQ R14, R14
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	XORQ R14, R14
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14
	ADDQ R13, (SP)
	ADCQ $0x00, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R13
	ADDQ R14, 8(SP)
	ADCQ $0x00, R13

	// | w8
	ADDQ R13, 16(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(DI), R8

	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ R14, R14
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14

	// | w2
	XORQ R13, R13
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R13
	ADDQ R14, R10
	ADCQ $0x00, R13

	// | w3
	XORQ R14, R14
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14
	ADDQ R13, R11
	ADCQ $0x00, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	XORQ R14, R14
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14
	ADDQ R13, SI
	ADCQ $0x00, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R13
	ADDQ R14, (SP)
	ADCQ $0x00, R13

	// | w7
	XORQ R14, R14
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R14
	ADDQ R13, 8(SP)
	ADCQ $0x00, R14

	// | w8
	XORQ R13, R13
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, R13
	ADDQ R14, 16(SP)
	ADCQ $0x00, R13

	// | w9
	ADDQ R13, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(DI), R9

	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ R14, R14
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14

	// | w3
	XORQ R13, R13
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w4
	XORQ R14, R14
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14
	ADDQ R13, (SP)
	ADCQ $0x00, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R13
	ADDQ R14, 8(SP)
	ADCQ $0x00, R13

	// | w8
	XORQ R14, R14
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, R14
	ADDQ R13, 16(SP)
	ADCQ $0x00, R14

	// | w9
	XORQ R13, R13
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R13
	ADDQ R14, R8
	ADCQ $0x00, R13

	// | w10
	ADDQ R13, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(DI), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ R14, R14
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14

	// | w4
	XORQ R13, R13
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w5
	XORQ R14, R14
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14
	ADDQ R13, SI
	ADCQ $0x00, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R13
	ADDQ R14, (SP)
	ADCQ $0x00, R13

	// | w7
	XORQ R14, R14
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R14
	ADDQ R13, 8(SP)
	ADCQ $0x00, R14

	// | w8
	XORQ R13, R13
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, R13
	ADDQ R14, 16(SP)
	ADCQ $0x00, R13

	// | w9
	XORQ R14, R14
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w10
	XORQ R13, R13
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w11
	ADDQ R13, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 96(DI), R11

	// | u @ CX = w4 * inp
	MOVQ R12, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w4
	XORQ R14, R14
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14

	// | w5
	XORQ R13, R13
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w6
	XORQ R14, R14
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14
	ADDQ R13, (SP)
	ADCQ $0x00, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R13
	ADDQ R14, 8(SP)
	ADCQ $0x00, R13

	// | w8
	XORQ R14, R14
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, R14
	ADDQ R13, 16(SP)
	ADCQ $0x00, R14

	// | w9
	XORQ R13, R13
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R13
	ADDQ R14, R8
	ADCQ $0x00, R13

	// | w10
	XORQ R14, R14
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14
	ADDQ R13, R9
	ADCQ $0x00, R14

	// | w11
	XORQ R13, R13
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R13
	ADDQ R14, R10
	ADCQ $0x00, R13

	// | w12
	ADDQ R13, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 104(DI), R12

	// | u @ CX = w5 * inp
	MOVQ SI, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w5
	XORQ R14, R14
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R14

	// | w6
	XORQ R13, R13
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R13
	ADDQ R14, (SP)
	ADCQ $0x00, R13

	// | w7
	XORQ R14, R14
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R14
	ADDQ R13, 8(SP)
	ADCQ $0x00, R14

	// | w8
	XORQ R13, R13
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, R13
	ADDQ R14, 16(SP)
	ADCQ $0x00, R13

	// | w9
	XORQ R14, R14
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w10
	XORQ R13, R13
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w11
	XORQ R14, R14
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w12
	XORQ R13, R13
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w13
	ADDQ R13, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 112(DI), SI

	// | u @ CX = w6 * inp
	MOVQ (SP), AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w6
	XORQ R14, R14
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, R14

	// | w7
	XORQ R13, R13
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R13
	ADDQ R14, 8(SP)
	ADCQ $0x00, R13

	// | w8
	XORQ R14, R14
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, R14
	ADDQ R13, 16(SP)
	ADCQ $0x00, R14

	// | w9
	XORQ R13, R13
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R13
	ADDQ R14, R8
	ADCQ $0x00, R13

	// | w10
	XORQ R14, R14
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R14
	ADDQ R13, R9
	ADCQ $0x00, R14

	// | w11
	XORQ R13, R13
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R13
	ADDQ R14, R10
	ADCQ $0x00, R13

	// | w12
	XORQ R14, R14
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R14
	ADDQ R13, R11
	ADCQ $0x00, R14

	// | w13
	XORQ R13, R13
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADDQ R14, R12
	ADCQ $0x00, R13

	// | w14
	ADDQ R13, R15
	ADCQ R15, SI
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 120(DI), BX
	MOVQ BX, (SP)

	// | u @ CX = w7 * inp
	MOVQ 8(SP), AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w7
	XORQ R14, R14
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, R14

	// | w8
	XORQ R13, R13
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, R13
	ADDQ R14, 16(SP)
	ADCQ $0x00, R13

	// | w9
	XORQ R14, R14
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R14
	ADDQ R13, R8
	ADCQ $0x00, R14

	// | w10
	XORQ R13, R13
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R13
	ADDQ R14, R9
	ADCQ $0x00, R13

	// | w11
	XORQ R14, R14
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R14
	ADDQ R13, R10
	ADCQ $0x00, R14

	// | w12
	XORQ R13, R13
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R13
	ADDQ R14, R11
	ADCQ $0x00, R13

	// | w13
	XORQ R14, R14
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R14
	ADDQ R13, R12
	ADCQ $0x00, R14

	// | w14
	XORQ R13, R13
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, SI
	ADCQ DX, R13
	ADDQ R14, SI
	ADCQ $0x00, R13

	// | w15
	ADDQ R13, R15
	ADCQ R15, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 16(SP), R13
	SUBQ ·modulus8+0(SB), R13
	MOVQ R8, R14
	SBBQ ·modulus8+8(SB), R14
	MOVQ R9, CX
	SBBQ ·modulus8+16(SB), CX
	MOVQ R10, AX
	SBBQ ·modulus8+24(SB), AX
	MOVQ R11, DX
	SBBQ ·modulus8+32(SB), DX
	MOVQ R12, BX
	SBBQ ·modulus8+40(SB), BX
	MOVQ BX, 8(SP)
	MOVQ SI, BX
	SBBQ ·modulus8+48(SB), BX
	MOVQ BX, 24(SP)
	MOVQ (SP), BX
	SBBQ ·modulus8+56(SB), BX
	MOVQ BX, 32(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	MOVQ    16(SP), BX
	CMOVQCC R13, BX
	MOVQ    BX, 16(SP)
	MOVQ    16(SP), BX
	MOVQ    BX, (DI)
	CMOVQCC R14, R8
	MOVQ    R8, 8(DI)
	CMOVQCC CX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC AX, R10
	MOVQ    R10, 24(DI)
	CMOVQCC DX, R11
	MOVQ    R11, 32(DI)
	CMOVQCC 8(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 24(SP), SI
	MOVQ    SI, 48(DI)
	MOVQ    (SP), BX
	CMOVQCC 32(SP), BX
	MOVQ    BX, (SP)
	MOVQ    (SP), BX
	MOVQ    BX, 56(DI)
	RET

// func montmul8(c *Fe512, a *Fe512, b *Fe512)
TEXT ·montmul8(SB), $96-24
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b7
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 64(SP)

	// |
	// |
	// | u @ CX = w0 * inp
	MOVQ R8, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w0
	XORQ DI, DI
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1
	XORQ SI, SI
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2
	XORQ DI, DI
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	XORQ DI, DI
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w8
	ADDQ SI, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 8(SP), R8

	// | u @ CX = w1 * inp
	MOVQ R9, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w1
	XORQ DI, DI
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2
	XORQ SI, SI
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3
	XORQ DI, DI
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	XORQ DI, DI
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	XORQ DI, DI
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w8
	XORQ SI, SI
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9
	ADDQ SI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 16(SP), R9

	// | u @ CX = w2 * inp
	MOVQ R10, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w2
	XORQ DI, DI
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3
	XORQ SI, SI
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4
	XORQ DI, DI
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w8
	XORQ DI, DI
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9
	XORQ SI, SI
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w10
	ADDQ SI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 24(SP), R10

	// | u @ CX = w3 * inp
	MOVQ R11, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w3
	XORQ DI, DI
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4
	XORQ SI, SI
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5
	XORQ DI, DI
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	XORQ DI, DI
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w8
	XORQ SI, SI
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9
	XORQ DI, DI
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w10
	XORQ SI, SI
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w11
	ADDQ SI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 32(SP), R11

	// | u @ CX = w4 * inp
	MOVQ R12, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w4
	XORQ DI, DI
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5
	XORQ SI, SI
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6
	XORQ DI, DI
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14
	ADCQ $0x00, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w8
	XORQ DI, DI
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9
	XORQ SI, SI
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w10
	XORQ DI, DI
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w11
	XORQ SI, SI
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w12
	ADDQ SI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 40(SP), R12

	// | u @ CX = w5 * inp
	MOVQ R13, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w5
	XORQ DI, DI
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6
	XORQ SI, SI
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, SI
	ADDQ DI, R14
	ADCQ $0x00, SI

	// | w7
	XORQ DI, DI
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w8
	XORQ SI, SI
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9
	XORQ DI, DI
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w10
	XORQ SI, SI
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w11
	XORQ DI, DI
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w12
	XORQ SI, SI
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w13
	ADDQ SI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 48(SP), R13

	// | u @ CX = w6 * inp
	MOVQ R14, AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w6
	XORQ DI, DI
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI

	// | w7
	XORQ SI, SI
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w8
	XORQ DI, DI
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9
	XORQ SI, SI
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w10
	XORQ DI, DI
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w11
	XORQ SI, SI
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w12
	XORQ DI, DI
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w13
	XORQ SI, SI
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w14
	ADDQ SI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(SP), R14

	// | u @ CX = w7 * inp
	MOVQ 64(SP), AX
	MULQ ·inp8+0(SB)
	MOVQ AX, CX

	// | w7
	XORQ DI, DI
	MOVQ ·modulus8+0(SB), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI

	// | w8
	XORQ SI, SI
	MOVQ ·modulus8+8(SB), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9
	XORQ DI, DI
	MOVQ ·modulus8+16(SB), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w10
	XORQ SI, SI
	MOVQ ·modulus8+24(SB), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w11
	XORQ DI, DI
	MOVQ ·modulus8+32(SB), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w12
	XORQ SI, SI
	MOVQ ·modulus8+40(SB), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w13
	XORQ DI, DI
	MOVQ ·modulus8+48(SB), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w14
	XORQ SI, SI
	MOVQ ·modulus8+56(SB), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w15
	ADDQ SI, R15
	ADCQ R15, R14
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ (SP), CX
	SUBQ ·modulus8+0(SB), CX
	MOVQ R8, AX
	SBBQ ·modulus8+8(SB), AX
	MOVQ R9, DX
	SBBQ ·modulus8+16(SB), DX
	MOVQ R10, SI
	SBBQ ·modulus8+24(SB), SI
	MOVQ R11, BX
	SBBQ ·modulus8+32(SB), BX
	MOVQ BX, 64(SP)
	MOVQ R12, BX
	SBBQ ·modulus8+40(SB), BX
	MOVQ BX, 72(SP)
	MOVQ R13, BX
	SBBQ ·modulus8+48(SB), BX
	MOVQ BX, 80(SP)
	MOVQ R14, BX
	SBBQ ·modulus8+56(SB), BX
	MOVQ BX, 88(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	MOVQ    (SP), BX
	CMOVQCC CX, BX
	MOVQ    BX, (SP)
	MOVQ    (SP), BX
	MOVQ    BX, (DI)
	CMOVQCC AX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC DX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC SI, R10
	MOVQ    R10, 24(DI)
	CMOVQCC 64(SP), R11
	MOVQ    R11, 32(DI)
	CMOVQCC 72(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 80(SP), R13
	MOVQ    R13, 48(DI)
	CMOVQCC 88(SP), R14
	MOVQ    R14, 56(DI)
	RET

// func square8(c *[16]uint64, a *Fe512)
TEXT ·square8(SB), $0-16
	RET

// func montsquare8(c *Fe512, a *Fe512)
TEXT ·montsquare8(SB), $0-16
	RET
