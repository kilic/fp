// Code generated by command: go run main.go -output ./generic -opt D. DO NOT EDIT.

#include "textflag.h"

// func cpy1(dst *[1]uint64, src *[1]uint64)
TEXT ·cpy1(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	RET

// func eq1(a *[1]uint64, b *[1]uint64) bool
TEXT ·eq1(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp1(a *[1]uint64, b *[1]uint64) int8
TEXT ·cmp1(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add1(c *[1]uint64, a *[1]uint64, b *[1]uint64, p *[1]uint64)
TEXT ·add1(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, DX
	SUBQ (SI), DX
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC DX, CX
	MOVQ    CX, (DI)
	RET

// func addn1(a *[1]uint64, b *[1]uint64) uint64
TEXT ·addn1(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ AX, ret+16(FP)
	RET

// func double1(c *[1]uint64, a *[1]uint64, p *[1]uint64)
TEXT ·double1(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, DX
	SUBQ (SI), DX
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC DX, CX
	MOVQ    CX, (DI)
	RET

// func sub1(c *[1]uint64, a *[1]uint64, b *[1]uint64, p *[1]uint64)
TEXT ·sub1(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), DX
	CMOVQCC AX, DX

	// |
	MOVQ c+0(FP), DI
	ADDQ DX, CX
	MOVQ CX, (DI)
	RET

// func subn1(a *[1]uint64, b *[1]uint64) uint64
TEXT ·subn1(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg1(c *[1]uint64, a *[1]uint64, p *[1]uint64)
TEXT ·_neg1(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	RET

// func mul_two_1(a *[1]uint64)
TEXT ·mul_two_1(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RET

// func div_two_1(a *[1]uint64)
TEXT ·div_two_1(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, (DI)
	RET

// func cpy2(dst *[2]uint64, src *[2]uint64)
TEXT ·cpy2(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	RET

// func eq2(a *[2]uint64, b *[2]uint64) bool
TEXT ·eq2(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp2(a *[2]uint64, b *[2]uint64) int8
TEXT ·cmp2(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add2(c *[2]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64)
TEXT ·add2(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R8
	SUBQ (SI), R8
	MOVQ DX, R9
	SBBQ 8(SI), R9
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R8, CX
	MOVQ    CX, (DI)
	CMOVQCC R9, DX
	MOVQ    DX, 8(DI)
	RET

// func addn2(a *[2]uint64, b *[2]uint64) uint64
TEXT ·addn2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double2(c *[2]uint64, a *[2]uint64, p *[2]uint64)
TEXT ·double2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R8
	SUBQ (SI), R8
	MOVQ DX, R9
	SBBQ 8(SI), R9
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R8, CX
	MOVQ    CX, (DI)
	CMOVQCC R9, DX
	MOVQ    DX, 8(DI)
	RET

// func sub2(c *[2]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64)
TEXT ·sub2(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R8
	CMOVQCC AX, R8
	MOVQ    8(SI), R9
	CMOVQCC AX, R9

	// |
	MOVQ c+0(FP), DI
	ADDQ R8, CX
	MOVQ CX, (DI)
	ADCQ R9, DX
	MOVQ DX, 8(DI)
	RET

// func subn2(a *[2]uint64, b *[2]uint64) uint64
TEXT ·subn2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg2(c *[2]uint64, a *[2]uint64, p *[2]uint64)
TEXT ·_neg2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	RET

// func mul_two_2(a *[2]uint64)
TEXT ·mul_two_2(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RET

// func div_two_2(a *[2]uint64)
TEXT ·div_two_2(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul2(c *[2]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64, inp uint64)
TEXT ·mul2(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8
	ADCQ  $0x00, R9

	// |
	// |
	MOVQ 8(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ p+24(FP), R15

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9
	ADOXQ BX, R9
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, SI
	ADOXQ BX, SI
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R8, BX

	// | 

/* reduction 				*/

	MOVQ R9, AX
	SUBQ (R15), AX
	MOVQ SI, DX
	SBBQ 8(R15), DX
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R9
	MOVQ    R9, (BX)
	CMOVQCC DX, SI
	MOVQ    SI, 8(BX)
	RET

	// | 

/* end 				*/


// func mul_no_adx_bmi2_2(c *[2]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64, inp uint64)
TEXT ·mul_no_adx_bmi2_2(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R10

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, BX

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ p+24(FP), R15

	// | 

/*	i = 0				*/

	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI

	// | w1 @ R9
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ SI, R9

	// | w2 @ R10
	ADCQ DX, R10
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI

	// | w2 @ R10
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10

	// | w3 @ BX
	ADCQ DI, BX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R10, AX
	SUBQ (R15), AX
	MOVQ BX, DX
	SBBQ 8(R15), DX
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC AX, R10
	MOVQ    R10, (DI)
	CMOVQCC DX, BX
	MOVQ    BX, 8(DI)
	RET

	// | 

/* end 				*/


// func cpy3(dst *[3]uint64, src *[3]uint64)
TEXT ·cpy3(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	RET

// func eq3(a *[3]uint64, b *[3]uint64) bool
TEXT ·eq3(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp3(a *[3]uint64, b *[3]uint64) int8
TEXT ·cmp3(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add3(c *[3]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64)
TEXT ·add3(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R9
	SUBQ (SI), R9
	MOVQ DX, R10
	SBBQ 8(SI), R10
	MOVQ R8, R11
	SBBQ 16(SI), R11
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R9, CX
	MOVQ    CX, (DI)
	CMOVQCC R10, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R11, R8
	MOVQ    R8, 16(DI)
	RET

// func addn3(a *[3]uint64, b *[3]uint64) uint64
TEXT ·addn3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double3(c *[3]uint64, a *[3]uint64, p *[3]uint64)
TEXT ·double3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R9
	SUBQ (SI), R9
	MOVQ DX, R10
	SBBQ 8(SI), R10
	MOVQ R8, R11
	SBBQ 16(SI), R11
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R9, CX
	MOVQ    CX, (DI)
	CMOVQCC R10, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R11, R8
	MOVQ    R8, 16(DI)
	RET

// func sub3(c *[3]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64)
TEXT ·sub3(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R9
	CMOVQCC AX, R9
	MOVQ    8(SI), R10
	CMOVQCC AX, R10
	MOVQ    16(SI), R11
	CMOVQCC AX, R11

	// |
	MOVQ c+0(FP), DI
	ADDQ R9, CX
	MOVQ CX, (DI)
	ADCQ R10, DX
	MOVQ DX, 8(DI)
	ADCQ R11, R8
	MOVQ R8, 16(DI)
	RET

// func subn3(a *[3]uint64, b *[3]uint64) uint64
TEXT ·subn3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg3(c *[3]uint64, a *[3]uint64, p *[3]uint64)
TEXT ·_neg3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	RET

// func mul_two_3(a *[3]uint64)
TEXT ·mul_two_3(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RET

// func div_two_3(a *[3]uint64)
TEXT ·div_two_3(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul3(c *[3]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64, inp uint64)
TEXT ·mul3(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 16(DI), AX, R10
	ADCXQ AX, R9
	ADCQ  $0x00, R10

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R11, R11

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADOXQ R11, R11
	ADCXQ BX, R11

	// |
	// |
	MOVQ 16(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ p+24(FP), R15

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10
	ADOXQ BX, R10
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11
	ADOXQ BX, R11
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R8, R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, SI
	ADOXQ BX, SI
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R9, BX

	// | 

/* reduction 				*/

	MOVQ R10, AX
	SUBQ (R15), AX
	MOVQ R11, DX
	SBBQ 8(R15), DX
	MOVQ SI, DI
	SBBQ 16(R15), DI
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R10
	MOVQ    R10, (BX)
	CMOVQCC DX, R11
	MOVQ    R11, 8(BX)
	CMOVQCC DI, SI
	MOVQ    SI, 16(BX)
	RET

	// | 

/* end 				*/


// func mul_no_adx_bmi2_3(c *[3]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64, inp uint64)
TEXT ·mul_no_adx_bmi2_3(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ p+24(FP), R15

	// | 

/*	i = 0				*/

	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI

	// | w1 @ R9
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ SI, R9
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w2 @ R10
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10

	// | w3 @ R11
	ADCQ DX, R11
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI

	// | w2 @ R10
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w3 @ R11
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11

	// | w4 @ R12
	ADCQ DI, R12
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI

	// | w3 @ R11
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R12
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12

	// | w5 @ BX
	ADCQ DI, BX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R11, AX
	SUBQ (R15), AX
	MOVQ R12, DX
	SBBQ 8(R15), DX
	MOVQ BX, SI
	SBBQ 16(R15), SI
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC AX, R11
	MOVQ    R11, (DI)
	CMOVQCC DX, R12
	MOVQ    R12, 8(DI)
	CMOVQCC SI, BX
	MOVQ    BX, 16(DI)
	RET

	// | 

/* end 				*/


// func cpy4(dst *[4]uint64, src *[4]uint64)
TEXT ·cpy4(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	RET

// func eq4(a *[4]uint64, b *[4]uint64) bool
TEXT ·eq4(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp4(a *[4]uint64, b *[4]uint64) int8
TEXT ·cmp4(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64)
TEXT ·add4(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R10
	SUBQ (SI), R10
	MOVQ DX, R11
	SBBQ 8(SI), R11
	MOVQ R8, R12
	SBBQ 16(SI), R12
	MOVQ R9, R13
	SBBQ 24(SI), R13
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R10, CX
	MOVQ    CX, (DI)
	CMOVQCC R11, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R12, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R13, R9
	MOVQ    R9, 24(DI)
	RET

// func addn4(a *[4]uint64, b *[4]uint64) uint64
TEXT ·addn4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double4(c *[4]uint64, a *[4]uint64, p *[4]uint64)
TEXT ·double4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R10
	SUBQ (SI), R10
	MOVQ DX, R11
	SBBQ 8(SI), R11
	MOVQ R8, R12
	SBBQ 16(SI), R12
	MOVQ R9, R13
	SBBQ 24(SI), R13
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R10, CX
	MOVQ    CX, (DI)
	CMOVQCC R11, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R12, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R13, R9
	MOVQ    R9, 24(DI)
	RET

// func sub4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64)
TEXT ·sub4(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R10
	CMOVQCC AX, R10
	MOVQ    8(SI), R11
	CMOVQCC AX, R11
	MOVQ    16(SI), R12
	CMOVQCC AX, R12
	MOVQ    24(SI), R13
	CMOVQCC AX, R13

	// |
	MOVQ c+0(FP), DI
	ADDQ R10, CX
	MOVQ CX, (DI)
	ADCQ R11, DX
	MOVQ DX, 8(DI)
	ADCQ R12, R8
	MOVQ R8, 16(DI)
	ADCQ R13, R9
	MOVQ R9, 24(DI)
	RET

// func subn4(a *[4]uint64, b *[4]uint64) uint64
TEXT ·subn4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg4(c *[4]uint64, a *[4]uint64, p *[4]uint64)
TEXT ·_neg4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	RET

// func mul_two_4(a *[4]uint64)
TEXT ·mul_two_4(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RET

// func div_two_4(a *[4]uint64)
TEXT ·div_two_4(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64, inp uint64)
TEXT ·mul4(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 16(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 24(DI), AX, R11
	ADCXQ AX, R10
	ADCQ  $0x00, R11

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R12, R12

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADOXQ R12, R12
	ADCXQ BX, R12

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R13, R13

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADOXQ R13, R13
	ADCXQ BX, R13

	// |
	// |
	MOVQ 24(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ p+24(FP), R15

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11
	ADOXQ BX, R11
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12
	ADOXQ BX, R12
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R8, R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13
	ADOXQ BX, R13
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R9, R9

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, SI
	ADOXQ BX, SI
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R10, BX

	// | 

/* reduction 				*/

	MOVQ R11, AX
	SUBQ (R15), AX
	MOVQ R12, DX
	SBBQ 8(R15), DX
	MOVQ R13, DI
	SBBQ 16(R15), DI
	MOVQ SI, R8
	SBBQ 24(R15), R8
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R11
	MOVQ    R11, (BX)
	CMOVQCC DX, R12
	MOVQ    R12, 8(BX)
	CMOVQCC DI, R13
	MOVQ    R13, 16(BX)
	CMOVQCC R8, SI
	MOVQ    SI, 24(BX)
	RET

	// | 

/* end 				*/


// func mul_no_adx_bmi2_4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64, inp uint64)
TEXT ·mul_no_adx_bmi2_4(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ p+24(FP), R15

	// | 

/*	i = 0				*/

	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI

	// | w1 @ R9
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ SI, R9
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w2 @ R10
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w3 @ R11
	MOVQ 24(R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11

	// | w4 @ R12
	ADCQ DX, R12
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI

	// | w2 @ R10
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w3 @ R11
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R12
	MOVQ 24(R15), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12

	// | w5 @ R13
	ADCQ DI, R13
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI

	// | w3 @ R11
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R12
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R13
	MOVQ 24(R15), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13

	// | w6 @ R14
	ADCQ DI, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI

	// | w4 @ R12
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R13
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ SI, R13
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w6 @ R14
	MOVQ 24(R15), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14

	// | w7 @ BX
	ADCQ DI, BX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R12, AX
	SUBQ (R15), AX
	MOVQ R13, DX
	SBBQ 8(R15), DX
	MOVQ R14, SI
	SBBQ 16(R15), SI
	MOVQ BX, R8
	SBBQ 24(R15), R8
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC AX, R12
	MOVQ    R12, (DI)
	CMOVQCC DX, R13
	MOVQ    R13, 8(DI)
	CMOVQCC SI, R14
	MOVQ    R14, 16(DI)
	CMOVQCC R8, BX
	MOVQ    BX, 24(DI)
	RET

	// | 

/* end 				*/


// func cpy5(dst *[5]uint64, src *[5]uint64)
TEXT ·cpy5(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	RET

// func eq5(a *[5]uint64, b *[5]uint64) bool
TEXT ·eq5(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp5(a *[5]uint64, b *[5]uint64) int8
TEXT ·cmp5(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64)
TEXT ·add5(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R11
	SUBQ (SI), R11
	MOVQ DX, R12
	SBBQ 8(SI), R12
	MOVQ R8, R13
	SBBQ 16(SI), R13
	MOVQ R9, R14
	SBBQ 24(SI), R14
	MOVQ R10, R15
	SBBQ 32(SI), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R11, CX
	MOVQ    CX, (DI)
	CMOVQCC R12, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R13, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 24(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 32(DI)
	RET

// func addn5(a *[5]uint64, b *[5]uint64) uint64
TEXT ·addn5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double5(c *[5]uint64, a *[5]uint64, p *[5]uint64)
TEXT ·double5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R11
	SUBQ (SI), R11
	MOVQ DX, R12
	SBBQ 8(SI), R12
	MOVQ R8, R13
	SBBQ 16(SI), R13
	MOVQ R9, R14
	SBBQ 24(SI), R14
	MOVQ R10, R15
	SBBQ 32(SI), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R11, CX
	MOVQ    CX, (DI)
	CMOVQCC R12, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R13, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 24(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 32(DI)
	RET

// func sub5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64)
TEXT ·sub5(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R11
	CMOVQCC AX, R11
	MOVQ    8(SI), R12
	CMOVQCC AX, R12
	MOVQ    16(SI), R13
	CMOVQCC AX, R13
	MOVQ    24(SI), R14
	CMOVQCC AX, R14
	MOVQ    32(SI), R15
	CMOVQCC AX, R15

	// |
	MOVQ c+0(FP), DI
	ADDQ R11, CX
	MOVQ CX, (DI)
	ADCQ R12, DX
	MOVQ DX, 8(DI)
	ADCQ R13, R8
	MOVQ R8, 16(DI)
	ADCQ R14, R9
	MOVQ R9, 24(DI)
	ADCQ R15, R10
	MOVQ R10, 32(DI)
	RET

// func subn5(a *[5]uint64, b *[5]uint64) uint64
TEXT ·subn5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg5(c *[5]uint64, a *[5]uint64, p *[5]uint64)
TEXT ·_neg5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	RET

// func mul_two_5(a *[5]uint64)
TEXT ·mul_two_5(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RET

// func div_two_5(a *[5]uint64)
TEXT ·div_two_5(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64, inp uint64)
TEXT ·mul5(SB), NOSPLIT, $8-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 16(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 24(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 32(DI), AX, R12
	ADCXQ AX, R11
	ADCQ  $0x00, R12

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R13, R13

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADOXQ R13, R13
	ADCXQ BX, R13

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R14, R14

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ R14, R14
	ADCXQ BX, R14

	// |
	// |
	MOVQ 24(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 32(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ SI, (SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12
	ADOXQ BX, R12
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13
	ADOXQ BX, R13
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R8, R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, R14
	ADOXQ BX, R14
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R9, R9

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, R14

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R14
	ADCXQ DI, R15
	ADOXQ BX, R15
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R10, R10
	MOVQ  (SP), R10

	// |
	// |
	MOVQ  R11, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, R14

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R14
	ADCXQ DI, R15

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R15
	ADCXQ DI, R10
	ADOXQ BX, R10
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R11, BX

	// | 

/* reduction 				*/

	MOVQ R12, AX
	SUBQ (SI), AX
	MOVQ R13, DX
	SBBQ 8(SI), DX
	MOVQ R14, DI
	SBBQ 16(SI), DI
	MOVQ R15, R8
	SBBQ 24(SI), R8
	MOVQ R10, R9
	SBBQ 32(SI), R9
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R12
	MOVQ    R12, (BX)
	CMOVQCC DX, R13
	MOVQ    R13, 8(BX)
	CMOVQCC DI, R14
	MOVQ    R14, 16(BX)
	CMOVQCC R8, R15
	MOVQ    R15, 24(BX)
	CMOVQCC R9, R10
	MOVQ    R10, 32(BX)
	RET

	// | 

/* end 				*/


// func mul_no_adx_bmi2_5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64, inp uint64)
TEXT ·mul_no_adx_bmi2_5(SB), NOSPLIT, $8-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R9
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14
	MOVQ $0x00, R15

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, (SP)
	MOVQ DX, R8

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a4 * b0 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, R10
	ADCQ $0x00, BX

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13

	// | a4 * b1 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14

	// | a4 * b2 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15

	// | a4 * b3 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | 

/*	i = 4				*/

	// | b4 @ CX
	MOVQ 32(SI), CX
	MOVQ $0x00, BX

	// | a0 * b4 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX

	// | a1 * b4 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b4 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b4 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, BX

	// | a4 * b4 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ (SP), DI
	MOVQ R8, (SP)
	MOVQ p+24(FP), R8

	// | 

/*	i = 0				*/

	MOVQ DI, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, DI
	ADCQ DX, SI

	// | w1 @ (SP)
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ $0x00, DX
	ADDQ SI, (SP)
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w2 @ R9
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ SI, R9
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w3 @ R10
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R11
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11

	// | w5 @ R12
	ADCQ DX, R12
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI

	// | w2 @ R9
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ SI, R9
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w3 @ R10
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R11
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R12
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12

	// | w6 @ R13
	ADCQ DI, R13
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI

	// | w3 @ R10
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R11
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R12
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w6 @ R13
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13

	// | w7 @ R14
	ADCQ DI, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI

	// | w4 @ R11
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R12
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w6 @ R13
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ SI, R13
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w7 @ R14
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14

	// | w8 @ R15
	ADCQ DI, R15
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 4				*/

	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI

	// | w5 @ R12
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w6 @ R13
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ SI, R13
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w7 @ R14
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ SI, R14
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w8 @ R15
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, DI
	ADDQ SI, R15

	// | w9 @ BX
	ADCQ DI, BX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R12, AX
	SUBQ (R8), AX
	MOVQ R13, DX
	SBBQ 8(R8), DX
	MOVQ R14, SI
	SBBQ 16(R8), SI
	MOVQ R15, R9
	SBBQ 24(R8), R9
	MOVQ BX, R10
	SBBQ 32(R8), R10
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC AX, R12
	MOVQ    R12, (DI)
	CMOVQCC DX, R13
	MOVQ    R13, 8(DI)
	CMOVQCC SI, R14
	MOVQ    R14, 16(DI)
	CMOVQCC R9, R15
	MOVQ    R15, 24(DI)
	CMOVQCC R10, BX
	MOVQ    BX, 32(DI)
	RET

	// | 

/* end 				*/


// func cpy6(dst *[6]uint64, src *[6]uint64)
TEXT ·cpy6(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	RET

// func eq6(a *[6]uint64, b *[6]uint64) bool
TEXT ·eq6(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp6(a *[6]uint64, b *[6]uint64) int8
TEXT ·cmp6(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64)
TEXT ·add6(SB), NOSPLIT, $16-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R12
	SUBQ (SI), R12
	MOVQ DX, R13
	SBBQ 8(SI), R13
	MOVQ R8, R14
	SBBQ 16(SI), R14
	MOVQ R9, R15
	SBBQ 24(SI), R15
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, (SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 8(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, CX
	MOVQ    CX, (DI)
	CMOVQCC R13, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R14, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R15, R9
	MOVQ    R9, 24(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 8(SP), R11
	MOVQ    R11, 40(DI)
	RET

// func addn6(a *[6]uint64, b *[6]uint64) uint64
TEXT ·addn6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double6(c *[6]uint64, a *[6]uint64, p *[6]uint64)
TEXT ·double6(SB), NOSPLIT, $16-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R12
	SUBQ (SI), R12
	MOVQ DX, R13
	SBBQ 8(SI), R13
	MOVQ R8, R14
	SBBQ 16(SI), R14
	MOVQ R9, R15
	SBBQ 24(SI), R15
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, (SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 8(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, CX
	MOVQ    CX, (DI)
	CMOVQCC R13, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R14, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R15, R9
	MOVQ    R9, 24(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 8(SP), R11
	MOVQ    R11, 40(DI)
	RET

// func sub6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64)
TEXT ·sub6(SB), NOSPLIT, $16-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R12
	CMOVQCC AX, R12
	MOVQ    8(SI), R13
	CMOVQCC AX, R13
	MOVQ    16(SI), R14
	CMOVQCC AX, R14
	MOVQ    24(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 32(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 8(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R12, CX
	MOVQ CX, (DI)
	ADCQ R13, DX
	MOVQ DX, 8(DI)
	ADCQ R14, R8
	MOVQ R8, 16(DI)
	ADCQ R15, R9
	MOVQ R9, 24(DI)
	ADCQ (SP), R10
	MOVQ R10, 32(DI)
	ADCQ 8(SP), R11
	MOVQ R11, 40(DI)
	RET

// func subn6(a *[6]uint64, b *[6]uint64) uint64
TEXT ·subn6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg6(c *[6]uint64, a *[6]uint64, p *[6]uint64)
TEXT ·_neg6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	RET

// func mul_two_6(a *[6]uint64)
TEXT ·mul_two_6(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RET

// func div_two_6(a *[6]uint64)
TEXT ·div_two_6(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64, inp uint64)
TEXT ·mul6(SB), NOSPLIT, $24-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, CX
	MOVQ  AX, (SP)

	// |
	MULXQ 8(DI), AX, R8
	ADCXQ AX, CX

	// |
	MULXQ 16(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 24(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 32(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 40(DI), AX, R12
	ADCXQ AX, R11
	ADCQ  $0x00, R12

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R13, R13

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8
	MOVQ  CX, 8(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R12
	ADOXQ R13, R13
	ADCXQ BX, R13

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R14, R14

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ R14, R14
	ADCXQ BX, R14

	// |
	// |
	MOVQ 24(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 32(SI), DX
	XORQ CX, CX

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ CX, CX
	ADCXQ BX, CX

	// |
	// |
	MOVQ 40(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, CX
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ (SP), DI
	MOVQ 8(SP), BX
	MOVQ R15, (SP)
	MOVQ CX, 8(SP)
	MOVQ SI, 16(SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  CX, CX
	MOVQ  DI, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, DI
	ADCXQ R15, BX

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, BX
	ADCXQ R15, R8

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12
	ADOXQ CX, R12
	ADCXQ CX, CX
	XORQ  DI, DI

	// |
	// |
	MOVQ  BX, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, BX
	ADCXQ R15, R8

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13
	ADOXQ CX, R13
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  BX, BX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14
	ADOXQ CX, R14
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  R8, R8
	MOVQ  (SP), R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R14
	ADCXQ R15, R8
	ADOXQ CX, R8
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  R9, R9
	MOVQ  8(SP), R9

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R14
	ADCXQ R15, R8

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9
	ADOXQ CX, R9
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  R10, R10
	MOVQ  16(SP), R10

	// |
	// |
	MOVQ  R11, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R14
	ADCXQ R15, R8

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10
	ADOXQ CX, R10
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	ADOXQ R11, CX

	// | 

/* reduction 				*/

	MOVQ R12, DX
	SUBQ (SI), DX
	MOVQ R13, BX
	SBBQ 8(SI), BX
	MOVQ R14, DI
	SBBQ 16(SI), DI
	MOVQ R8, R11
	SBBQ 24(SI), R11
	MOVQ R9, R15
	SBBQ 32(SI), R15
	MOVQ R10, AX
	SBBQ 40(SI), AX
	MOVQ AX, (SP)
	SBBQ $0x00, CX

	// |
	MOVQ    c+0(FP), CX
	CMOVQCC DX, R12
	MOVQ    R12, (CX)
	CMOVQCC BX, R13
	MOVQ    R13, 8(CX)
	CMOVQCC DI, R14
	MOVQ    R14, 16(CX)
	CMOVQCC R11, R8
	MOVQ    R8, 24(CX)
	CMOVQCC R15, R9
	MOVQ    R9, 32(CX)
	CMOVQCC (SP), R10
	MOVQ    R10, 40(CX)
	RET

	// | 

/* end 				*/


// func mul_no_adx_bmi2_6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64, inp uint64)
TEXT ·mul_no_adx_bmi2_6(SB), NOSPLIT, $24-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R9
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14
	MOVQ $0x00, R15

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, (SP)
	MOVQ DX, R8

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a4 * b0 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a5 * b0 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, R10
	ADCQ $0x00, BX
	MOVQ R8, 8(SP)
	MOVQ $0x00, R8

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b1 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14

	// | a5 * b1 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX
	MOVQ R9, 16(SP)
	MOVQ $0x00, R9

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b2 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15

	// | a5 * b2 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b3 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8

	// | a5 * b3 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8

	// | 

/*	i = 4				*/

	// | b4 @ CX
	MOVQ 32(SI), CX
	MOVQ $0x00, BX

	// | a0 * b4 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX

	// | a1 * b4 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b4 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b4 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b4 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9

	// | a5 * b4 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | 

/*	i = 5				*/

	// | b5 @ CX
	MOVQ 40(SI), CX
	MOVQ $0x00, BX

	// | a0 * b5 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, BX

	// | a1 * b5 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b5 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b5 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b5 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, BX

	// | a5 * b5 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ (SP), DI
	MOVQ 8(SP), SI
	MOVQ 16(SP), CX
	MOVQ R8, (SP)
	MOVQ R9, 8(SP)
	MOVQ BX, 16(SP)
	MOVQ p+24(FP), BX

	// | 

/*	i = 0				*/

	MOVQ DI, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, DI
	ADCQ DX, R8

	// | w1 @ SI
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R8, SI
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w2 @ CX
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R8, CX
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w3 @ R10
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R8, R10
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w4 @ R11
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R8, R11
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12

	// | w6 @ R13
	ADCQ DX, R13
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ SI, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, SI
	ADCQ DX, R8

	// | w2 @ CX
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R8, CX
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w3 @ R10
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R8, R10
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w4 @ R11
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R8, R11
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ R8, R13

	// | w7 @ R14
	ADCQ DI, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ CX, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, CX
	ADCQ DX, R8

	// | w3 @ R10
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R8, R10
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w4 @ R11
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R8, R11
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R8, R13
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w7 @ R14
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ R8, R14

	// | w8 @ R15
	ADCQ DI, R15
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, R10
	ADCQ DX, R8

	// | w4 @ R11
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R8, R11
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R8, R13
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w7 @ R14
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R8, R14
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w8 @ R15
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, R15
	ADCQ DX, DI
	ADDQ R8, R15

	// | w9 @ SI
	// | move to emptied register
	MOVQ (SP), SI
	ADCQ DI, SI
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 4				*/

	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R8, R13
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w7 @ R14
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R8, R14
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w8 @ R15
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R8, R15
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w9 @ SI
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, SI
	ADCQ DX, DI
	ADDQ R8, SI

	// | w10 @ CX
	// | move to emptied register
	MOVQ 8(SP), CX
	ADCQ DI, CX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 5				*/

	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R8, R13
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w7 @ R14
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R8, R14
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w8 @ R15
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R8, R15
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w9 @ SI
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R8, SI
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w10 @ CX
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, CX
	ADCQ DX, DI
	ADDQ R8, CX

	// | w11 @ R10
	// | move to emptied register
	MOVQ 16(SP), R10
	ADCQ DI, R10
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R13, DX
	SUBQ (BX), DX
	MOVQ R14, R8
	SBBQ 8(BX), R8
	MOVQ R15, R9
	SBBQ 16(BX), R9
	MOVQ SI, R11
	SBBQ 24(BX), R11
	MOVQ CX, R12
	SBBQ 32(BX), R12
	MOVQ R10, AX
	SBBQ 40(BX), AX
	MOVQ AX, (SP)
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC DX, R13
	MOVQ    R13, (DI)
	CMOVQCC R8, R14
	MOVQ    R14, 8(DI)
	CMOVQCC R9, R15
	MOVQ    R15, 16(DI)
	CMOVQCC R11, SI
	MOVQ    SI, 24(DI)
	CMOVQCC R12, CX
	MOVQ    CX, 32(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 40(DI)
	RET

	// | 

/* end 				*/


// func cpy7(dst *[7]uint64, src *[7]uint64)
TEXT ·cpy7(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	RET

// func eq7(a *[7]uint64, b *[7]uint64) bool
TEXT ·eq7(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp7(a *[7]uint64, b *[7]uint64) int8
TEXT ·cmp7(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64)
TEXT ·add7(SB), NOSPLIT, $32-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R13
	SUBQ (SI), R13
	MOVQ DX, R14
	SBBQ 8(SI), R14
	MOVQ R8, R15
	SBBQ 16(SI), R15
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, (SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 24(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, CX
	MOVQ    CX, (DI)
	CMOVQCC R14, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R15, R8
	MOVQ    R8, 16(DI)
	CMOVQCC (SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 24(SP), R12
	MOVQ    R12, 48(DI)
	RET

// func addn7(a *[7]uint64, b *[7]uint64) uint64
TEXT ·addn7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double7(c *[7]uint64, a *[7]uint64, p *[7]uint64)
TEXT ·double7(SB), NOSPLIT, $32-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R13
	SUBQ (SI), R13
	MOVQ DX, R14
	SBBQ 8(SI), R14
	MOVQ R8, R15
	SBBQ 16(SI), R15
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, (SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 24(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, CX
	MOVQ    CX, (DI)
	CMOVQCC R14, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R15, R8
	MOVQ    R8, 16(DI)
	CMOVQCC (SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 24(SP), R12
	MOVQ    R12, 48(DI)
	RET

// func sub7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64)
TEXT ·sub7(SB), NOSPLIT, $32-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R13
	CMOVQCC AX, R13
	MOVQ    8(SI), R14
	CMOVQCC AX, R14
	MOVQ    16(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 24(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 24(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R13, CX
	MOVQ CX, (DI)
	ADCQ R14, DX
	MOVQ DX, 8(DI)
	ADCQ R15, R8
	MOVQ R8, 16(DI)
	ADCQ (SP), R9
	MOVQ R9, 24(DI)
	ADCQ 8(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 16(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 24(SP), R12
	MOVQ R12, 48(DI)
	RET

// func subn7(a *[7]uint64, b *[7]uint64) uint64
TEXT ·subn7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg7(c *[7]uint64, a *[7]uint64, p *[7]uint64)
TEXT ·_neg7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	RET

// func mul_two_7(a *[7]uint64)
TEXT ·mul_two_7(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RET

// func div_two_7(a *[7]uint64)
TEXT ·div_two_7(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64, inp uint64)
TEXT ·mul7(SB), NOSPLIT, $40-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, CX
	MOVQ  AX, (SP)

	// |
	MULXQ 8(DI), AX, R8
	ADCXQ AX, CX

	// |
	MULXQ 16(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 24(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 32(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 40(DI), AX, R12
	ADCXQ AX, R11

	// |
	MULXQ 48(DI), AX, R13
	ADCXQ AX, R12
	ADCQ  $0x00, R13

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R14, R14

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8
	MOVQ  CX, 8(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ R14, R14
	ADCXQ BX, R14

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9
	MOVQ  R8, 16(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 24(SI), DX
	XORQ CX, CX

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10
	MOVQ  R9, 24(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ CX, CX
	ADCXQ BX, CX

	// |
	// |
	MOVQ 32(SI), DX
	XORQ R8, R8

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, CX
	ADOXQ R8, R8
	ADCXQ BX, R8

	// |
	// |
	MOVQ 40(SI), DX
	XORQ R9, R9

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R8
	ADOXQ R9, R9
	ADCXQ BX, R9

	// |
	// |
	MOVQ 48(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R9
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ (SP), DI
	MOVQ 8(SP), BX
	MOVQ R15, (SP)
	MOVQ 16(SP), R15
	MOVQ CX, 8(SP)
	MOVQ 24(SP), CX
	MOVQ R8, 16(SP)
	MOVQ R9, 24(SP)
	MOVQ SI, 32(SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  R9, R9
	MOVQ  DI, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, DI
	ADCXQ R8, BX

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13
	ADOXQ R9, R13
	ADCXQ R9, R9
	XORQ  DI, DI

	// |
	// |
	MOVQ  BX, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14
	ADOXQ R9, R14
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  BX, BX
	MOVQ  (SP), BX

	// |
	// |
	MOVQ  R15, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX
	ADOXQ R9, BX
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  R15, R15
	MOVQ  8(SP), R15

	// |
	// |
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15
	ADOXQ R9, R15
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  CX, CX
	MOVQ  16(SP), CX

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX
	ADOXQ R9, CX
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  R10, R10
	MOVQ  24(SP), R10

	// |
	// |
	MOVQ  R11, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10
	ADOXQ R9, R10
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  R11, R11
	MOVQ  32(SP), R11

	// |
	// |
	MOVQ  R12, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11
	ADOXQ R9, R11
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	ADOXQ R12, R9

	// | 

/* reduction 				*/

	MOVQ R13, DX
	SUBQ (SI), DX
	MOVQ R14, DI
	SBBQ 8(SI), DI
	MOVQ BX, R8
	SBBQ 16(SI), R8
	MOVQ R15, R12
	SBBQ 24(SI), R12
	MOVQ CX, AX
	SBBQ 32(SI), AX
	MOVQ AX, (SP)
	MOVQ R10, AX
	SBBQ 40(SI), AX
	MOVQ AX, 8(SP)
	MOVQ R11, AX
	SBBQ 48(SI), AX
	MOVQ AX, 16(SP)
	SBBQ $0x00, R9

	// |
	MOVQ    c+0(FP), R9
	CMOVQCC DX, R13
	MOVQ    R13, (R9)
	CMOVQCC DI, R14
	MOVQ    R14, 8(R9)
	CMOVQCC R8, BX
	MOVQ    BX, 16(R9)
	CMOVQCC R12, R15
	MOVQ    R15, 24(R9)
	CMOVQCC (SP), CX
	MOVQ    CX, 32(R9)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 40(R9)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 48(R9)
	RET

	// | 

/* end 				*/


// func mul_no_adx_bmi2_7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64, inp uint64)
TEXT ·mul_no_adx_bmi2_7(SB), NOSPLIT, $40-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R9
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14
	MOVQ $0x00, R15

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, (SP)
	MOVQ DX, R8

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a4 * b0 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a5 * b0 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a6 * b0 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, R10
	ADCQ $0x00, BX
	MOVQ R8, 8(SP)
	MOVQ $0x00, R8

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b1 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b1 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15

	// | a6 * b1 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX
	MOVQ R9, 16(SP)
	MOVQ $0x00, R9

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b2 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b2 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8

	// | a6 * b2 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX
	MOVQ R10, 24(SP)
	MOVQ $0x00, R10

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b3 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b3 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9

	// | a6 * b3 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | 

/*	i = 4				*/

	// | b4 @ CX
	MOVQ 32(SI), CX
	MOVQ $0x00, BX

	// | a0 * b4 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX
	MOVQ R11, 32(SP)
	MOVQ $0x00, R11

	// | a1 * b4 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b4 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b4 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b4 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b4 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10

	// | a6 * b4 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | 

/*	i = 5				*/

	// | b5 @ CX
	MOVQ 40(SI), CX
	MOVQ $0x00, BX

	// | a0 * b5 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, BX

	// | a1 * b5 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b5 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b5 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b5 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b5 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11

	// | a6 * b5 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | 

/*	i = 6				*/

	// | b6 @ CX
	MOVQ 48(SI), CX
	MOVQ $0x00, BX

	// | a0 * b6 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, BX

	// | a1 * b6 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b6 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b6 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b6 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b6 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, BX

	// | a6 * b6 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ (SP), DI
	MOVQ 8(SP), SI
	MOVQ 16(SP), CX
	MOVQ R8, (SP)
	MOVQ 24(SP), R8
	MOVQ R9, 8(SP)
	MOVQ 32(SP), R9
	MOVQ R10, 16(SP)
	MOVQ R11, 24(SP)
	MOVQ BX, 32(SP)
	MOVQ p+24(FP), BX

	// | 

/*	i = 0				*/

	MOVQ DI, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, DI
	ADCQ DX, R10

	// | w1 @ SI
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R10, SI
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w2 @ CX
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R10, CX
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w3 @ R8
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R10, R8
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w4 @ R9
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R10, R9
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13

	// | w7 @ R14
	ADCQ DX, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ SI, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ DX, R10

	// | w2 @ CX
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R10, CX
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w3 @ R8
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R10, R8
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w4 @ R9
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R10, R9
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ R10, R14

	// | w8 @ R15
	ADCQ DI, R15
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ CX, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ DX, R10

	// | w3 @ R8
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R10, R8
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w4 @ R9
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R10, R9
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ DX, DI
	ADDQ R10, R15

	// | w9 @ SI
	// | move to emptied register
	MOVQ (SP), SI
	ADCQ DI, SI
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ DX, R10

	// | w4 @ R9
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R10, R9
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R10, R15
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w9 @ SI
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ DX, DI
	ADDQ R10, SI

	// | w10 @ CX
	// | move to emptied register
	MOVQ 8(SP), CX
	ADCQ DI, CX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 4				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R10, R15
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w9 @ SI
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R10, SI
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w10 @ CX
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ DX, DI
	ADDQ R10, CX

	// | w11 @ R8
	// | move to emptied register
	MOVQ 16(SP), R8
	ADCQ DI, R8
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 5				*/

	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R10, R15
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w9 @ SI
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R10, SI
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w10 @ CX
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R10, CX
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w11 @ R8
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ R10, R8

	// | w12 @ R9
	// | move to emptied register
	MOVQ 24(SP), R9
	ADCQ DI, R9
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 6				*/

	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R10, R15
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w9 @ SI
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R10, SI
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w10 @ CX
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R10, CX
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w11 @ R8
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R10, R8
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w12 @ R9
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ R10, R9

	// | w13 @ R12
	// | move to emptied register
	MOVQ 32(SP), R12
	ADCQ DI, R12
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R14, DX
	SUBQ (BX), DX
	MOVQ R15, R10
	SBBQ 8(BX), R10
	MOVQ SI, R11
	SBBQ 16(BX), R11
	MOVQ CX, R13
	SBBQ 24(BX), R13
	MOVQ R8, AX
	SBBQ 32(BX), AX
	MOVQ AX, (SP)
	MOVQ R9, AX
	SBBQ 40(BX), AX
	MOVQ AX, 8(SP)
	MOVQ R12, AX
	SBBQ 48(BX), AX
	MOVQ AX, 16(SP)
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC DX, R14
	MOVQ    R14, (DI)
	CMOVQCC R10, R15
	MOVQ    R15, 8(DI)
	CMOVQCC R11, SI
	MOVQ    SI, 16(DI)
	CMOVQCC R13, CX
	MOVQ    CX, 24(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 32(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 40(DI)
	CMOVQCC 16(SP), R12
	MOVQ    R12, 48(DI)
	RET

	// | 

/* end 				*/


// func cpy8(dst *[8]uint64, src *[8]uint64)
TEXT ·cpy8(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	RET

// func eq8(a *[8]uint64, b *[8]uint64) bool
TEXT ·eq8(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp8(a *[8]uint64, b *[8]uint64) int8
TEXT ·cmp8(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64)
TEXT ·add8(SB), NOSPLIT, $48-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R14
	SUBQ (SI), R14
	MOVQ DX, R15
	SBBQ 8(SI), R15
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, (SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 40(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, CX
	MOVQ    CX, (DI)
	CMOVQCC R15, DX
	MOVQ    DX, 8(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 40(SP), R13
	MOVQ    R13, 56(DI)
	RET

// func addn8(a *[8]uint64, b *[8]uint64) uint64
TEXT ·addn8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double8(c *[8]uint64, a *[8]uint64, p *[8]uint64)
TEXT ·double8(SB), NOSPLIT, $48-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R14
	SUBQ (SI), R14
	MOVQ DX, R15
	SBBQ 8(SI), R15
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, (SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 40(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, CX
	MOVQ    CX, (DI)
	CMOVQCC R15, DX
	MOVQ    DX, 8(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 40(SP), R13
	MOVQ    R13, 56(DI)
	RET

// func sub8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64)
TEXT ·sub8(SB), NOSPLIT, $48-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R14
	CMOVQCC AX, R14
	MOVQ    8(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 16(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 24(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 40(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R14, CX
	MOVQ CX, (DI)
	ADCQ R15, DX
	MOVQ DX, 8(DI)
	ADCQ (SP), R8
	MOVQ R8, 16(DI)
	ADCQ 8(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 16(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 24(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 32(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 40(SP), R13
	MOVQ R13, 56(DI)
	RET

// func subn8(a *[8]uint64, b *[8]uint64) uint64
TEXT ·subn8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg8(c *[8]uint64, a *[8]uint64, p *[8]uint64)
TEXT ·_neg8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	RET

// func mul_two_8(a *[8]uint64)
TEXT ·mul_two_8(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RET

// func div_two_8(a *[8]uint64)
TEXT ·div_two_8(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64, inp uint64)
TEXT ·mul8(SB), NOSPLIT, $56-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, CX
	MOVQ  AX, (SP)

	// |
	MULXQ 8(DI), AX, R8
	ADCXQ AX, CX

	// |
	MULXQ 16(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 24(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 32(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 40(DI), AX, R12
	ADCXQ AX, R11

	// |
	MULXQ 48(DI), AX, R13
	ADCXQ AX, R12

	// |
	MULXQ 56(DI), AX, R14
	ADCXQ AX, R13
	ADCQ  $0x00, R14

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8
	MOVQ  CX, 8(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 16(SI), DX
	XORQ CX, CX

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9
	MOVQ  R8, 16(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ CX, CX
	ADCXQ BX, CX

	// |
	// |
	MOVQ 24(SI), DX
	XORQ R8, R8

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10
	MOVQ  R9, 24(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, CX
	ADOXQ R8, R8
	ADCXQ BX, R8

	// |
	// |
	MOVQ 32(SI), DX
	XORQ R9, R9

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11
	MOVQ  R10, 32(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R8
	ADOXQ R9, R9
	ADCXQ BX, R9

	// |
	// |
	MOVQ 40(SI), DX
	XORQ R10, R10

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12
	MOVQ  R11, 40(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R9
	ADOXQ R10, R10
	ADCXQ BX, R10

	// |
	// |
	MOVQ 48(SI), DX
	XORQ R11, R11

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R10
	ADOXQ R11, R11
	ADCXQ BX, R11

	// |
	// |
	MOVQ 56(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R11
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ (SP), DI
	MOVQ 8(SP), BX
	MOVQ R15, (SP)
	MOVQ 16(SP), R15
	MOVQ CX, 8(SP)
	MOVQ 24(SP), CX
	MOVQ R8, 16(SP)
	MOVQ 32(SP), R8
	MOVQ R9, 24(SP)
	MOVQ 40(SP), R9
	MOVQ R10, 32(SP)
	MOVQ R11, 40(SP)
	MOVQ SI, 48(SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  R11, R11
	MOVQ  DI, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14
	ADOXQ R11, R14
	ADCXQ R11, R11
	XORQ  DI, DI
	MOVQ  (SP), DI

	// |
	// |
	MOVQ  BX, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI
	ADOXQ R11, DI
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  BX, BX
	MOVQ  8(SP), BX

	// |
	// |
	MOVQ  R15, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX
	ADOXQ R11, BX
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R15, R15
	MOVQ  16(SP), R15

	// |
	// |
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15
	ADOXQ R11, R15
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  CX, CX
	MOVQ  24(SP), CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX
	ADOXQ R11, CX
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R8, R8
	MOVQ  32(SP), R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8
	ADOXQ R11, R8
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R9, R9
	MOVQ  40(SP), R9

	// |
	// |
	MOVQ  R12, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9
	ADOXQ R11, R9
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R12, R12
	MOVQ  48(SP), R12

	// |
	// |
	MOVQ  R13, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12
	ADOXQ R11, R12
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	ADOXQ R13, R11

	// | 

/* reduction 				*/

	MOVQ R14, DX
	SUBQ (SI), DX
	MOVQ DI, R10
	SBBQ 8(SI), R10
	MOVQ BX, R13
	SBBQ 16(SI), R13
	MOVQ R15, AX
	SBBQ 24(SI), AX
	MOVQ AX, (SP)
	MOVQ CX, AX
	SBBQ 32(SI), AX
	MOVQ AX, 8(SP)
	MOVQ R8, AX
	SBBQ 40(SI), AX
	MOVQ AX, 16(SP)
	MOVQ R9, AX
	SBBQ 48(SI), AX
	MOVQ AX, 24(SP)
	MOVQ R12, AX
	SBBQ 56(SI), AX
	MOVQ AX, 32(SP)
	SBBQ $0x00, R11

	// |
	MOVQ    c+0(FP), R11
	CMOVQCC DX, R14
	MOVQ    R14, (R11)
	CMOVQCC R10, DI
	MOVQ    DI, 8(R11)
	CMOVQCC R13, BX
	MOVQ    BX, 16(R11)
	CMOVQCC (SP), R15
	MOVQ    R15, 24(R11)
	CMOVQCC 8(SP), CX
	MOVQ    CX, 32(R11)
	CMOVQCC 16(SP), R8
	MOVQ    R8, 40(R11)
	CMOVQCC 24(SP), R9
	MOVQ    R9, 48(R11)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 56(R11)
	RET

	// | 

/* end 				*/


// func mul_no_adx_bmi2_8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64, inp uint64)
TEXT ·mul_no_adx_bmi2_8(SB), NOSPLIT, $56-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R9
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14
	MOVQ $0x00, R15

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, (SP)
	MOVQ DX, R8

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a4 * b0 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a5 * b0 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a6 * b0 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a7 * b0 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, R10
	ADCQ $0x00, BX
	MOVQ R8, 8(SP)
	MOVQ $0x00, R8

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b1 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b1 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b1 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8

	// | a7 * b1 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX
	MOVQ R9, 16(SP)
	MOVQ $0x00, R9

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b2 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b2 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b2 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9

	// | a7 * b2 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX
	MOVQ R10, 24(SP)
	MOVQ $0x00, R10

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b3 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b3 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b3 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10

	// | a7 * b3 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | 

/*	i = 4				*/

	// | b4 @ CX
	MOVQ 32(SI), CX
	MOVQ $0x00, BX

	// | a0 * b4 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX
	MOVQ R11, 32(SP)
	MOVQ $0x00, R11

	// | a1 * b4 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b4 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b4 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b4 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b4 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b4 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11

	// | a7 * b4 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | 

/*	i = 5				*/

	// | b5 @ CX
	MOVQ 40(SI), CX
	MOVQ $0x00, BX

	// | a0 * b5 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, BX
	MOVQ R12, 40(SP)
	MOVQ $0x00, R12

	// | a1 * b5 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b5 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b5 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b5 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b5 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b5 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12

	// | a7 * b5 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | 

/*	i = 6				*/

	// | b6 @ CX
	MOVQ 48(SI), CX
	MOVQ $0x00, BX

	// | a0 * b6 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, BX
	MOVQ R13, 48(SP)
	MOVQ $0x00, R13

	// | a1 * b6 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b6 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b6 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b6 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b6 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b6 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13

	// | a7 * b6 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | 

/*	i = 7				*/

	// | b7 @ CX
	MOVQ 56(SI), CX
	MOVQ $0x00, BX

	// | a0 * b7 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, R8
	ADCQ $0x00, BX

	// | a1 * b7 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b7 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b7 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b7 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b7 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b7 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, BX

	// | a7 * b7 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ (SP), DI
	MOVQ 8(SP), SI
	MOVQ 16(SP), CX
	MOVQ R8, (SP)
	MOVQ 24(SP), R8
	MOVQ R9, 8(SP)
	MOVQ 32(SP), R9
	MOVQ R10, 16(SP)
	MOVQ 40(SP), R10
	MOVQ R11, 24(SP)
	MOVQ 48(SP), R11
	MOVQ R12, 32(SP)
	MOVQ R13, 40(SP)
	MOVQ BX, 48(SP)
	MOVQ p+24(FP), BX

	// | 

/*	i = 0				*/

	MOVQ DI, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, DI
	ADCQ DX, R12

	// | w1 @ SI
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w2 @ CX
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w3 @ R8
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w4 @ R9
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14

	// | w8 @ R15
	ADCQ DX, R15
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ SI, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ DX, R12

	// | w2 @ CX
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w3 @ R8
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w4 @ R9
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ DX, DI
	ADDQ R12, R15

	// | w9 @ SI
	// | move to emptied register
	MOVQ (SP), SI
	ADCQ DI, SI
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ CX, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ DX, R12

	// | w3 @ R8
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w4 @ R9
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ DX, DI
	ADDQ R12, SI

	// | w10 @ CX
	// | move to emptied register
	MOVQ 8(SP), CX
	ADCQ DI, CX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ DX, R12

	// | w4 @ R9
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ DX, DI
	ADDQ R12, CX

	// | w11 @ R8
	// | move to emptied register
	MOVQ 16(SP), R8
	ADCQ DI, R8
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 4				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w11 @ R8
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ R12, R8

	// | w12 @ R9
	// | move to emptied register
	MOVQ 24(SP), R9
	ADCQ DI, R9
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 5				*/

	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w11 @ R8
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w12 @ R9
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ R12, R9

	// | w13 @ R10
	// | move to emptied register
	MOVQ 32(SP), R10
	ADCQ DI, R10
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 6				*/

	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w11 @ R8
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w12 @ R9
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w13 @ R10
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ R12, R10

	// | w14 @ R11
	// | move to emptied register
	MOVQ 40(SP), R11
	ADCQ DI, R11
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 7				*/

	MOVQ R14, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w11 @ R8
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w12 @ R9
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w13 @ R10
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w14 @ R11
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ R12, R11

	// | w15 @ R14
	// | move to emptied register
	MOVQ 48(SP), R14
	ADCQ DI, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R15, DX
	SUBQ (BX), DX
	MOVQ SI, R12
	SBBQ 8(BX), R12
	MOVQ CX, R13
	SBBQ 16(BX), R13
	MOVQ R8, AX
	SBBQ 24(BX), AX
	MOVQ AX, (SP)
	MOVQ R9, AX
	SBBQ 32(BX), AX
	MOVQ AX, 8(SP)
	MOVQ R10, AX
	SBBQ 40(BX), AX
	MOVQ AX, 16(SP)
	MOVQ R11, AX
	SBBQ 48(BX), AX
	MOVQ AX, 24(SP)
	MOVQ R14, AX
	SBBQ 56(BX), AX
	MOVQ AX, 32(SP)
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC DX, R15
	MOVQ    R15, (DI)
	CMOVQCC R12, SI
	MOVQ    SI, 8(DI)
	CMOVQCC R13, CX
	MOVQ    CX, 16(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 24(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 32(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 40(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 48(DI)
	CMOVQCC 32(SP), R14
	MOVQ    R14, 56(DI)
	RET

	// | 

/* end 				*/


// func mul1(c *[1]uint64, a *[1]uint64, b *[1]uint64, p *[1]uint64, inp uint64)
TEXT ·mul1(SB), NOSPLIT, $0-40

/* inputs 								*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

/* multiplication 				*/

	MOVQ (SI), DX
	MULXQ (DI), R8, R9

/* montgommery reduction	*/

	MOVQ p+24(FP), R15
  MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

  MULXQ (R15), AX, DI
  ADDQ AX, R8
	ADCQ DI, R9
	ADCQ $0x00, R8

/* modular reduction 			*/

	MOVQ R9, AX
	SUBQ (R15), AX
	SBBQ $0x00, R8

/* out 										*/

	MOVQ    c+0(FP), DI
	CMOVQCC AX, R9
	MOVQ    R9, (DI)
	RET

/* end 				*/

// func mul_no_adx_bmi2_1(c *[1]uint64, a *[1]uint64, b *[1]uint64, p *[1]uint64, inp uint64)
TEXT ·mul_no_adx_bmi2_1(SB), NOSPLIT, $0-40

/* inputs 										*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// | 

/* multiplication 						*/

	MOVQ (SI), CX
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

/* montgommery reduction 			*/

	MOVQ p+24(FP), R15

	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, R8

/* modular reduction 				*/

	MOVQ R9, AX
	SUBQ (R15), AX
	SBBQ $0x00, R8

/* out 											*/

	MOVQ    c+0(FP), DI
	CMOVQCC AX, R9
	MOVQ    R9, (DI)
	RET

/* end 											*/

TEXT ·is_even(SB), NOSPLIT, $0-9
	MOVQ a+0(FP), DI
	MOVB $0x00, ret+8(FP)
	MOVQ 0(DI), AX
	TESTQ $1, AX 
	JNZ ret
	MOVB $0x01, ret+8(FP)
ret:
	RET
