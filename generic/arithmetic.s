// Code generated by command: go run main.go -output generic_1_unsafe/ -opt D. DO NOT EDIT.

#include "textflag.h"

// func cpy4(dst *[4]uint64, src *[4]uint64)
TEXT ·cpy4(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	RET

// func eq4(a *[4]uint64, b *[4]uint64) bool
TEXT ·eq4(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp4(a *[4]uint64, b *[4]uint64) int8
TEXT ·cmp4(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64)
TEXT ·add4(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R10
	SUBQ (SI), R10
	MOVQ DX, R11
	SBBQ 8(SI), R11
	MOVQ R8, R12
	SBBQ 16(SI), R12
	MOVQ R9, R13
	SBBQ 24(SI), R13
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R10, CX
	MOVQ    CX, (DI)
	CMOVQCC R11, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R12, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R13, R9
	MOVQ    R9, 24(DI)
	RET

// func addn4(a *[4]uint64, b *[4]uint64) uint64
TEXT ·addn4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double4(c *[4]uint64, a *[4]uint64, p *[4]uint64)
TEXT ·double4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R10
	SUBQ (SI), R10
	MOVQ DX, R11
	SBBQ 8(SI), R11
	MOVQ R8, R12
	SBBQ 16(SI), R12
	MOVQ R9, R13
	SBBQ 24(SI), R13
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R10, CX
	MOVQ    CX, (DI)
	CMOVQCC R11, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R12, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R13, R9
	MOVQ    R9, 24(DI)
	RET

// func sub4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64)
TEXT ·sub4(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R10
	CMOVQCC AX, R10
	MOVQ    8(SI), R11
	CMOVQCC AX, R11
	MOVQ    16(SI), R12
	CMOVQCC AX, R12
	MOVQ    24(SI), R13
	CMOVQCC AX, R13

	// |
	MOVQ c+0(FP), DI
	ADDQ R10, CX
	MOVQ CX, (DI)
	ADCQ R11, DX
	MOVQ DX, 8(DI)
	ADCQ R12, R8
	MOVQ R8, 16(DI)
	ADCQ R13, R9
	MOVQ R9, 24(DI)
	RET

// func subn4(a *[4]uint64, b *[4]uint64) uint64
TEXT ·subn4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg4(c *[4]uint64, a *[4]uint64, p *[4]uint64)
TEXT ·_neg4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	RET

// func mul4(c *[8]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64, inp uint64)
TEXT ·mul4(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14
	MOVQ $0x00, R15

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ p+24(FP), R15

	// | 

/*	i = 0				*/

	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI

	// | w1 @ R9
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ SI, R9
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w2 @ R10
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w3 @ R11
	MOVQ 24(R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11

	// | w4 @ R12
	ADCQ DX, R12
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI

	// | w2 @ R10
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w3 @ R11
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R12
	MOVQ 24(R15), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12

	// | w5 @ R13
	ADCQ DI, R13
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI

	// | w3 @ R11
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R12
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R13
	MOVQ 24(R15), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13

	// | w6 @ R14
	ADCQ DI, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R15), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI

	// | w4 @ R12
	MOVQ 8(R15), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R13
	MOVQ 16(R15), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ SI, R13
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w6 @ R14
	MOVQ 24(R15), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14

	// | w7 @ BX
	ADCQ DI, BX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R12, AX
	SUBQ (R15), AX
	MOVQ R13, DX
	SBBQ 8(R15), DX
	MOVQ R14, SI
	SBBQ 16(R15), SI
	MOVQ BX, R8
	SBBQ 24(R15), R8
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC AX, R12
	MOVQ    R12, (DI)
	CMOVQCC DX, R13
	MOVQ    R13, 8(DI)
	CMOVQCC SI, R14
	MOVQ    R14, 16(DI)
	CMOVQCC R8, BX
	MOVQ    BX, 24(DI)
	RET

	// | 

/* end 				*/


// func mul_two_4(a *[4]uint64)
TEXT ·mul_two_4(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RET

// func div_two_4(a *[4]uint64)
TEXT ·div_two_4(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func cpy5(dst *[5]uint64, src *[5]uint64)
TEXT ·cpy5(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	RET

// func eq5(a *[5]uint64, b *[5]uint64) bool
TEXT ·eq5(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp5(a *[5]uint64, b *[5]uint64) int8
TEXT ·cmp5(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64)
TEXT ·add5(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R11
	SUBQ (SI), R11
	MOVQ DX, R12
	SBBQ 8(SI), R12
	MOVQ R8, R13
	SBBQ 16(SI), R13
	MOVQ R9, R14
	SBBQ 24(SI), R14
	MOVQ R10, R15
	SBBQ 32(SI), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R11, CX
	MOVQ    CX, (DI)
	CMOVQCC R12, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R13, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 24(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 32(DI)
	RET

// func addn5(a *[5]uint64, b *[5]uint64) uint64
TEXT ·addn5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double5(c *[5]uint64, a *[5]uint64, p *[5]uint64)
TEXT ·double5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R11
	SUBQ (SI), R11
	MOVQ DX, R12
	SBBQ 8(SI), R12
	MOVQ R8, R13
	SBBQ 16(SI), R13
	MOVQ R9, R14
	SBBQ 24(SI), R14
	MOVQ R10, R15
	SBBQ 32(SI), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R11, CX
	MOVQ    CX, (DI)
	CMOVQCC R12, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R13, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 24(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 32(DI)
	RET

// func sub5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64)
TEXT ·sub5(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R11
	CMOVQCC AX, R11
	MOVQ    8(SI), R12
	CMOVQCC AX, R12
	MOVQ    16(SI), R13
	CMOVQCC AX, R13
	MOVQ    24(SI), R14
	CMOVQCC AX, R14
	MOVQ    32(SI), R15
	CMOVQCC AX, R15

	// |
	MOVQ c+0(FP), DI
	ADDQ R11, CX
	MOVQ CX, (DI)
	ADCQ R12, DX
	MOVQ DX, 8(DI)
	ADCQ R13, R8
	MOVQ R8, 16(DI)
	ADCQ R14, R9
	MOVQ R9, 24(DI)
	ADCQ R15, R10
	MOVQ R10, 32(DI)
	RET

// func subn5(a *[5]uint64, b *[5]uint64) uint64
TEXT ·subn5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg5(c *[5]uint64, a *[5]uint64, p *[5]uint64)
TEXT ·_neg5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	RET

// func mul5(c *[10]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64, inp uint64)
TEXT ·mul5(SB), NOSPLIT, $8-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R9
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14
	MOVQ $0x00, R15

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, (SP)
	MOVQ DX, R8

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a4 * b0 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, R10
	ADCQ $0x00, BX

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13

	// | a4 * b1 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14

	// | a4 * b2 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15

	// | a4 * b3 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | 

/*	i = 4				*/

	// | b4 @ CX
	MOVQ 32(SI), CX
	MOVQ $0x00, BX

	// | a0 * b4 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX

	// | a1 * b4 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b4 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b4 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, BX

	// | a4 * b4 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ (SP), DI
	MOVQ R8, (SP)
	MOVQ p+24(FP), R8

	// | 

/*	i = 0				*/

	MOVQ DI, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, DI
	ADCQ DX, SI

	// | w1 @ (SP)
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ $0x00, DX
	ADDQ SI, (SP)
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w2 @ R9
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ SI, R9
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w3 @ R10
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R11
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11

	// | w5 @ R12
	ADCQ DX, R12
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI

	// | w2 @ R9
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ SI, R9
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w3 @ R10
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R11
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R12
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12

	// | w6 @ R13
	ADCQ DI, R13
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI

	// | w3 @ R10
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ SI, R10
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w4 @ R11
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R12
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w6 @ R13
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13

	// | w7 @ R14
	ADCQ DI, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI

	// | w4 @ R11
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ SI, R11
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w5 @ R12
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w6 @ R13
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ SI, R13
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w7 @ R14
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ SI, R14

	// | w8 @ R15
	ADCQ DI, R15
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 4				*/

	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX
	MOVQ $0x00, SI

	// |
	MOVQ (R8), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI

	// | w5 @ R12
	MOVQ 8(R8), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ SI, R12
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w6 @ R13
	MOVQ 16(R8), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ SI, R13
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w7 @ R14
	MOVQ 24(R8), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ SI, R14
	MOVQ $0x00, SI
	ADCQ DX, SI

	// | w8 @ R15
	MOVQ 32(R8), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, DI
	ADDQ SI, R15

	// | w9 @ BX
	ADCQ DI, BX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R12, AX
	SUBQ (R8), AX
	MOVQ R13, DX
	SBBQ 8(R8), DX
	MOVQ R14, SI
	SBBQ 16(R8), SI
	MOVQ R15, R9
	SBBQ 24(R8), R9
	MOVQ BX, R10
	SBBQ 32(R8), R10
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC AX, R12
	MOVQ    R12, (DI)
	CMOVQCC DX, R13
	MOVQ    R13, 8(DI)
	CMOVQCC SI, R14
	MOVQ    R14, 16(DI)
	CMOVQCC R9, R15
	MOVQ    R15, 24(DI)
	CMOVQCC R10, BX
	MOVQ    BX, 32(DI)
	RET

	// | 

/* end 				*/


// func mul_two_5(a *[5]uint64)
TEXT ·mul_two_5(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RET

// func div_two_5(a *[5]uint64)
TEXT ·div_two_5(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func cpy6(dst *[6]uint64, src *[6]uint64)
TEXT ·cpy6(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	RET

// func eq6(a *[6]uint64, b *[6]uint64) bool
TEXT ·eq6(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp6(a *[6]uint64, b *[6]uint64) int8
TEXT ·cmp6(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64)
TEXT ·add6(SB), NOSPLIT, $16-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R12
	SUBQ (SI), R12
	MOVQ DX, R13
	SBBQ 8(SI), R13
	MOVQ R8, R14
	SBBQ 16(SI), R14
	MOVQ R9, R15
	SBBQ 24(SI), R15
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, (SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 8(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, CX
	MOVQ    CX, (DI)
	CMOVQCC R13, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R14, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R15, R9
	MOVQ    R9, 24(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 8(SP), R11
	MOVQ    R11, 40(DI)
	RET

// func addn6(a *[6]uint64, b *[6]uint64) uint64
TEXT ·addn6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double6(c *[6]uint64, a *[6]uint64, p *[6]uint64)
TEXT ·double6(SB), NOSPLIT, $16-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R12
	SUBQ (SI), R12
	MOVQ DX, R13
	SBBQ 8(SI), R13
	MOVQ R8, R14
	SBBQ 16(SI), R14
	MOVQ R9, R15
	SBBQ 24(SI), R15
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, (SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 8(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, CX
	MOVQ    CX, (DI)
	CMOVQCC R13, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R14, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R15, R9
	MOVQ    R9, 24(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 8(SP), R11
	MOVQ    R11, 40(DI)
	RET

// func sub6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64)
TEXT ·sub6(SB), NOSPLIT, $16-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R12
	CMOVQCC AX, R12
	MOVQ    8(SI), R13
	CMOVQCC AX, R13
	MOVQ    16(SI), R14
	CMOVQCC AX, R14
	MOVQ    24(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 32(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 8(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R12, CX
	MOVQ CX, (DI)
	ADCQ R13, DX
	MOVQ DX, 8(DI)
	ADCQ R14, R8
	MOVQ R8, 16(DI)
	ADCQ R15, R9
	MOVQ R9, 24(DI)
	ADCQ (SP), R10
	MOVQ R10, 32(DI)
	ADCQ 8(SP), R11
	MOVQ R11, 40(DI)
	RET

// func subn6(a *[6]uint64, b *[6]uint64) uint64
TEXT ·subn6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg6(c *[6]uint64, a *[6]uint64, p *[6]uint64)
TEXT ·_neg6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	RET

// func mul6(c *[12]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64, inp uint64)
TEXT ·mul6(SB), NOSPLIT, $24-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R9
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14
	MOVQ $0x00, R15

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, (SP)
	MOVQ DX, R8

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a4 * b0 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a5 * b0 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, R10
	ADCQ $0x00, BX
	MOVQ R8, 8(SP)
	MOVQ $0x00, R8

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b1 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14

	// | a5 * b1 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX
	MOVQ R9, 16(SP)
	MOVQ $0x00, R9

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b2 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15

	// | a5 * b2 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b3 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8

	// | a5 * b3 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8

	// | 

/*	i = 4				*/

	// | b4 @ CX
	MOVQ 32(SI), CX
	MOVQ $0x00, BX

	// | a0 * b4 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX

	// | a1 * b4 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b4 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b4 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b4 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9

	// | a5 * b4 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | 

/*	i = 5				*/

	// | b5 @ CX
	MOVQ 40(SI), CX
	MOVQ $0x00, BX

	// | a0 * b5 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, BX

	// | a1 * b5 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b5 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b5 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b5 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, BX

	// | a5 * b5 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ (SP), DI
	MOVQ 8(SP), SI
	MOVQ 16(SP), CX
	MOVQ R8, (SP)
	MOVQ R9, 8(SP)
	MOVQ BX, 16(SP)
	MOVQ p+24(FP), BX

	// | 

/*	i = 0				*/

	MOVQ DI, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, DI
	ADCQ DX, R8

	// | w1 @ SI
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R8, SI
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w2 @ CX
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R8, CX
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w3 @ R10
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R8, R10
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w4 @ R11
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R8, R11
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12

	// | w6 @ R13
	ADCQ DX, R13
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ SI, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, SI
	ADCQ DX, R8

	// | w2 @ CX
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R8, CX
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w3 @ R10
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R8, R10
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w4 @ R11
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R8, R11
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ R8, R13

	// | w7 @ R14
	ADCQ DI, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ CX, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, CX
	ADCQ DX, R8

	// | w3 @ R10
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R8, R10
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w4 @ R11
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R8, R11
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R8, R13
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w7 @ R14
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ R8, R14

	// | w8 @ R15
	ADCQ DI, R15
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, R10
	ADCQ DX, R8

	// | w4 @ R11
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R8, R11
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R8, R13
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w7 @ R14
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R8, R14
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w8 @ R15
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, R15
	ADCQ DX, DI
	ADDQ R8, R15

	// | w9 @ SI
	// | move to emptied register
	MOVQ (SP), SI
	ADCQ DI, SI
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 4				*/

	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, R11
	ADCQ DX, R8

	// | w5 @ R12
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R8, R12
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R8, R13
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w7 @ R14
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R8, R14
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w8 @ R15
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R8, R15
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w9 @ SI
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, SI
	ADCQ DX, DI
	ADDQ R8, SI

	// | w10 @ CX
	// | move to emptied register
	MOVQ 8(SP), CX
	ADCQ DI, CX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 5				*/

	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, R9
	MOVQ $0x00, R8

	// |
	MOVQ (BX), AX
	MULQ R9
	ADDQ AX, R12
	ADCQ DX, R8

	// | w6 @ R13
	MOVQ 8(BX), AX
	MULQ R9
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R8, R13
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w7 @ R14
	MOVQ 16(BX), AX
	MULQ R9
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R8, R14
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w8 @ R15
	MOVQ 24(BX), AX
	MULQ R9
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R8, R15
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w9 @ SI
	MOVQ 32(BX), AX
	MULQ R9
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R8, SI
	MOVQ $0x00, R8
	ADCQ DX, R8

	// | w10 @ CX
	MOVQ 40(BX), AX
	MULQ R9
	ADDQ AX, CX
	ADCQ DX, DI
	ADDQ R8, CX

	// | w11 @ R10
	// | move to emptied register
	MOVQ 16(SP), R10
	ADCQ DI, R10
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R13, DX
	SUBQ (BX), DX
	MOVQ R14, R8
	SBBQ 8(BX), R8
	MOVQ R15, R9
	SBBQ 16(BX), R9
	MOVQ SI, R11
	SBBQ 24(BX), R11
	MOVQ CX, R12
	SBBQ 32(BX), R12
	MOVQ R10, AX
	SBBQ 40(BX), AX
	MOVQ AX, (SP)
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC DX, R13
	MOVQ    R13, (DI)
	CMOVQCC R8, R14
	MOVQ    R14, 8(DI)
	CMOVQCC R9, R15
	MOVQ    R15, 16(DI)
	CMOVQCC R11, SI
	MOVQ    SI, 24(DI)
	CMOVQCC R12, CX
	MOVQ    CX, 32(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 40(DI)
	RET

	// | 

/* end 				*/


// func mul_two_6(a *[6]uint64)
TEXT ·mul_two_6(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RET

// func div_two_6(a *[6]uint64)
TEXT ·div_two_6(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func cpy7(dst *[7]uint64, src *[7]uint64)
TEXT ·cpy7(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	RET

// func eq7(a *[7]uint64, b *[7]uint64) bool
TEXT ·eq7(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp7(a *[7]uint64, b *[7]uint64) int8
TEXT ·cmp7(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64)
TEXT ·add7(SB), NOSPLIT, $32-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R13
	SUBQ (SI), R13
	MOVQ DX, R14
	SBBQ 8(SI), R14
	MOVQ R8, R15
	SBBQ 16(SI), R15
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, (SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 24(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, CX
	MOVQ    CX, (DI)
	CMOVQCC R14, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R15, R8
	MOVQ    R8, 16(DI)
	CMOVQCC (SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 24(SP), R12
	MOVQ    R12, 48(DI)
	RET

// func addn7(a *[7]uint64, b *[7]uint64) uint64
TEXT ·addn7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double7(c *[7]uint64, a *[7]uint64, p *[7]uint64)
TEXT ·double7(SB), NOSPLIT, $32-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R13
	SUBQ (SI), R13
	MOVQ DX, R14
	SBBQ 8(SI), R14
	MOVQ R8, R15
	SBBQ 16(SI), R15
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, (SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 24(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, CX
	MOVQ    CX, (DI)
	CMOVQCC R14, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R15, R8
	MOVQ    R8, 16(DI)
	CMOVQCC (SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 24(SP), R12
	MOVQ    R12, 48(DI)
	RET

// func sub7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64)
TEXT ·sub7(SB), NOSPLIT, $32-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R13
	CMOVQCC AX, R13
	MOVQ    8(SI), R14
	CMOVQCC AX, R14
	MOVQ    16(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 24(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 24(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R13, CX
	MOVQ CX, (DI)
	ADCQ R14, DX
	MOVQ DX, 8(DI)
	ADCQ R15, R8
	MOVQ R8, 16(DI)
	ADCQ (SP), R9
	MOVQ R9, 24(DI)
	ADCQ 8(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 16(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 24(SP), R12
	MOVQ R12, 48(DI)
	RET

// func subn7(a *[7]uint64, b *[7]uint64) uint64
TEXT ·subn7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg7(c *[7]uint64, a *[7]uint64, p *[7]uint64)
TEXT ·_neg7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	RET

// func mul7(c *[14]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64, inp uint64)
TEXT ·mul7(SB), NOSPLIT, $40-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R9
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14
	MOVQ $0x00, R15

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, (SP)
	MOVQ DX, R8

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a4 * b0 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a5 * b0 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a6 * b0 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, R10
	ADCQ $0x00, BX
	MOVQ R8, 8(SP)
	MOVQ $0x00, R8

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b1 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b1 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15

	// | a6 * b1 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX
	MOVQ R9, 16(SP)
	MOVQ $0x00, R9

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b2 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b2 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8

	// | a6 * b2 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX
	MOVQ R10, 24(SP)
	MOVQ $0x00, R10

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b3 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b3 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9

	// | a6 * b3 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | 

/*	i = 4				*/

	// | b4 @ CX
	MOVQ 32(SI), CX
	MOVQ $0x00, BX

	// | a0 * b4 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX
	MOVQ R11, 32(SP)
	MOVQ $0x00, R11

	// | a1 * b4 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b4 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b4 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b4 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b4 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10

	// | a6 * b4 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | 

/*	i = 5				*/

	// | b5 @ CX
	MOVQ 40(SI), CX
	MOVQ $0x00, BX

	// | a0 * b5 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, BX

	// | a1 * b5 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b5 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b5 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b5 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b5 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11

	// | a6 * b5 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | 

/*	i = 6				*/

	// | b6 @ CX
	MOVQ 48(SI), CX
	MOVQ $0x00, BX

	// | a0 * b6 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, BX

	// | a1 * b6 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b6 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b6 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b6 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b6 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, BX

	// | a6 * b6 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ (SP), DI
	MOVQ 8(SP), SI
	MOVQ 16(SP), CX
	MOVQ R8, (SP)
	MOVQ 24(SP), R8
	MOVQ R9, 8(SP)
	MOVQ 32(SP), R9
	MOVQ R10, 16(SP)
	MOVQ R11, 24(SP)
	MOVQ BX, 32(SP)
	MOVQ p+24(FP), BX

	// | 

/*	i = 0				*/

	MOVQ DI, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, DI
	ADCQ DX, R10

	// | w1 @ SI
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R10, SI
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w2 @ CX
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R10, CX
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w3 @ R8
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R10, R8
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w4 @ R9
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R10, R9
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13

	// | w7 @ R14
	ADCQ DX, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ SI, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ DX, R10

	// | w2 @ CX
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R10, CX
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w3 @ R8
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R10, R8
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w4 @ R9
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R10, R9
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ DX, DI
	ADDQ R10, R14

	// | w8 @ R15
	ADCQ DI, R15
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ CX, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ DX, R10

	// | w3 @ R8
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R10, R8
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w4 @ R9
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R10, R9
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ DX, DI
	ADDQ R10, R15

	// | w9 @ SI
	// | move to emptied register
	MOVQ (SP), SI
	ADCQ DI, SI
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ DX, R10

	// | w4 @ R9
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R10, R9
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R10, R15
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w9 @ SI
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ DX, DI
	ADDQ R10, SI

	// | w10 @ CX
	// | move to emptied register
	MOVQ 8(SP), CX
	ADCQ DI, CX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 4				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ DX, R10

	// | w5 @ R12
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ $0x00, DX
	ADDQ R10, R12
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R10, R15
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w9 @ SI
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R10, SI
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w10 @ CX
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ DX, DI
	ADDQ R10, CX

	// | w11 @ R8
	// | move to emptied register
	MOVQ 16(SP), R8
	ADCQ DI, R8
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 5				*/

	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, R12
	ADCQ DX, R10

	// | w6 @ R13
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ $0x00, DX
	ADDQ R10, R13
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R10, R15
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w9 @ SI
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R10, SI
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w10 @ CX
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R10, CX
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w11 @ R8
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ R10, R8

	// | w12 @ R9
	// | move to emptied register
	MOVQ 24(SP), R9
	ADCQ DI, R9
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 6				*/

	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, R11
	MOVQ $0x00, R10

	// |
	MOVQ (BX), AX
	MULQ R11
	ADDQ AX, R13
	ADCQ DX, R10

	// | w7 @ R14
	MOVQ 8(BX), AX
	MULQ R11
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R10, R14
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w8 @ R15
	MOVQ 16(BX), AX
	MULQ R11
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R10, R15
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w9 @ SI
	MOVQ 24(BX), AX
	MULQ R11
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R10, SI
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w10 @ CX
	MOVQ 32(BX), AX
	MULQ R11
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R10, CX
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w11 @ R8
	MOVQ 40(BX), AX
	MULQ R11
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R10, R8
	MOVQ $0x00, R10
	ADCQ DX, R10

	// | w12 @ R9
	MOVQ 48(BX), AX
	MULQ R11
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ R10, R9

	// | w13 @ R12
	// | move to emptied register
	MOVQ 32(SP), R12
	ADCQ DI, R12
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R14, DX
	SUBQ (BX), DX
	MOVQ R15, R10
	SBBQ 8(BX), R10
	MOVQ SI, R11
	SBBQ 16(BX), R11
	MOVQ CX, R13
	SBBQ 24(BX), R13
	MOVQ R8, AX
	SBBQ 32(BX), AX
	MOVQ AX, (SP)
	MOVQ R9, AX
	SBBQ 40(BX), AX
	MOVQ AX, 8(SP)
	MOVQ R12, AX
	SBBQ 48(BX), AX
	MOVQ AX, 16(SP)
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC DX, R14
	MOVQ    R14, (DI)
	CMOVQCC R10, R15
	MOVQ    R15, 8(DI)
	CMOVQCC R11, SI
	MOVQ    SI, 16(DI)
	CMOVQCC R13, CX
	MOVQ    CX, 24(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 32(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 40(DI)
	CMOVQCC 16(SP), R12
	MOVQ    R12, 48(DI)
	RET

	// | 

/* end 				*/


// func mul_two_7(a *[7]uint64)
TEXT ·mul_two_7(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RET

// func div_two_7(a *[7]uint64)
TEXT ·div_two_7(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func cpy8(dst *[8]uint64, src *[8]uint64)
TEXT ·cpy8(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	RET

// func eq8(a *[8]uint64, b *[8]uint64) bool
TEXT ·eq8(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp8(a *[8]uint64, b *[8]uint64) int8
TEXT ·cmp8(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64)
TEXT ·add8(SB), NOSPLIT, $48-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R14
	SUBQ (SI), R14
	MOVQ DX, R15
	SBBQ 8(SI), R15
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, (SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 40(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, CX
	MOVQ    CX, (DI)
	CMOVQCC R15, DX
	MOVQ    DX, 8(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 40(SP), R13
	MOVQ    R13, 56(DI)
	RET

// func addn8(a *[8]uint64, b *[8]uint64) uint64
TEXT ·addn8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double8(c *[8]uint64, a *[8]uint64, p *[8]uint64)
TEXT ·double8(SB), NOSPLIT, $48-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R14
	SUBQ (SI), R14
	MOVQ DX, R15
	SBBQ 8(SI), R15
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, (SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 40(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, CX
	MOVQ    CX, (DI)
	CMOVQCC R15, DX
	MOVQ    DX, 8(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 40(SP), R13
	MOVQ    R13, 56(DI)
	RET

// func sub8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64)
TEXT ·sub8(SB), NOSPLIT, $48-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R14
	CMOVQCC AX, R14
	MOVQ    8(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 16(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 24(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 40(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R14, CX
	MOVQ CX, (DI)
	ADCQ R15, DX
	MOVQ DX, 8(DI)
	ADCQ (SP), R8
	MOVQ R8, 16(DI)
	ADCQ 8(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 16(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 24(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 32(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 40(SP), R13
	MOVQ R13, 56(DI)
	RET

// func subn8(a *[8]uint64, b *[8]uint64) uint64
TEXT ·subn8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg8(c *[8]uint64, a *[8]uint64, p *[8]uint64)
TEXT ·_neg8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	RET

// func mul8(c *[16]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64, inp uint64)
TEXT ·mul8(SB), NOSPLIT, $56-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	MOVQ $0x00, R9
	MOVQ $0x00, R10
	MOVQ $0x00, R11
	MOVQ $0x00, R12
	MOVQ $0x00, R13
	MOVQ $0x00, R14
	MOVQ $0x00, R15

	// | 

/*	i = 0				*/

	// | b0 @ CX
	MOVQ (SI), CX

	// | a0 * b0 
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, (SP)
	MOVQ DX, R8

	// | a1 * b0 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | a2 * b0 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a3 * b0 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a4 * b0 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a5 * b0 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a6 * b0 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a7 * b0 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | 

/*	i = 1				*/

	// | b1 @ CX
	MOVQ 8(SI), CX
	MOVQ $0x00, BX

	// | a0 * b1 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ $0x00, R10
	ADCQ $0x00, BX
	MOVQ R8, 8(SP)
	MOVQ $0x00, R8

	// | a1 * b1 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b1 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b1 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b1 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b1 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b1 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8

	// | a7 * b1 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8

	// | 

/*	i = 2				*/

	// | b2 @ CX
	MOVQ 16(SI), CX
	MOVQ $0x00, BX

	// | a0 * b2 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, BX
	MOVQ R9, 16(SP)
	MOVQ $0x00, R9

	// | a1 * b2 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b2 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b2 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b2 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b2 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b2 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9

	// | a7 * b2 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9

	// | 

/*	i = 3				*/

	// | b3 @ CX
	MOVQ 24(SI), CX
	MOVQ $0x00, BX

	// | a0 * b3 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, BX
	MOVQ R10, 24(SP)
	MOVQ $0x00, R10

	// | a1 * b3 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b3 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b3 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b3 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b3 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b3 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10

	// | a7 * b3 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | 

/*	i = 4				*/

	// | b4 @ CX
	MOVQ 32(SI), CX
	MOVQ $0x00, BX

	// | a0 * b4 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, BX
	MOVQ R11, 32(SP)
	MOVQ $0x00, R11

	// | a1 * b4 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ BX, R14
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b4 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b4 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b4 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b4 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b4 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11

	// | a7 * b4 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | 

/*	i = 5				*/

	// | b5 @ CX
	MOVQ 40(SI), CX
	MOVQ $0x00, BX

	// | a0 * b5 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, BX
	MOVQ R12, 40(SP)
	MOVQ $0x00, R12

	// | a1 * b5 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ BX, R15
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b5 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b5 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b5 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b5 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b5 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12

	// | a7 * b5 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | 

/*	i = 6				*/

	// | b6 @ CX
	MOVQ 48(SI), CX
	MOVQ $0x00, BX

	// | a0 * b6 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, BX
	MOVQ R13, 48(SP)
	MOVQ $0x00, R13

	// | a1 * b6 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ BX, R8
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b6 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b6 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b6 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b6 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b6 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13

	// | a7 * b6 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | 

/*	i = 7				*/

	// | b7 @ CX
	MOVQ 56(SI), CX
	MOVQ $0x00, BX

	// | a0 * b7 
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, R8
	ADCQ $0x00, BX

	// | a1 * b7 
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, R8
	ADCQ BX, R9
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a2 * b7 
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, R9
	ADCQ BX, R10
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a3 * b7 
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ BX, R11
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a4 * b7 
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ BX, R12
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a5 * b7 
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ BX, R13
	MOVQ $0x00, BX
	ADCQ $0x00, BX

	// | a6 * b7 
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, BX

	// | a7 * b7 
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, BX

	// | 

/* swap 				*/

	MOVQ (SP), DI
	MOVQ 8(SP), SI
	MOVQ 16(SP), CX
	MOVQ R8, (SP)
	MOVQ 24(SP), R8
	MOVQ R9, 8(SP)
	MOVQ 32(SP), R9
	MOVQ R10, 16(SP)
	MOVQ 40(SP), R10
	MOVQ R11, 24(SP)
	MOVQ 48(SP), R11
	MOVQ R12, 32(SP)
	MOVQ R13, 40(SP)
	MOVQ BX, 48(SP)
	MOVQ p+24(FP), BX

	// | 

/*	i = 0				*/

	MOVQ DI, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, DI
	ADCQ DX, R12

	// | w1 @ SI
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w2 @ CX
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w3 @ R8
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w4 @ R9
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14

	// | w8 @ R15
	ADCQ DX, R15
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 1				*/

	MOVQ SI, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ DX, R12

	// | w2 @ CX
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w3 @ R8
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w4 @ R9
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ DX, DI
	ADDQ R12, R15

	// | w9 @ SI
	// | move to emptied register
	MOVQ (SP), SI
	ADCQ DI, SI
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 2				*/

	MOVQ CX, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ DX, R12

	// | w3 @ R8
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w4 @ R9
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ DX, DI
	ADDQ R12, SI

	// | w10 @ CX
	// | move to emptied register
	MOVQ 8(SP), CX
	ADCQ DI, CX
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 3				*/

	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ DX, R12

	// | w4 @ R9
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ DX, DI
	ADDQ R12, CX

	// | w11 @ R8
	// | move to emptied register
	MOVQ 16(SP), R8
	ADCQ DI, R8
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 4				*/

	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ DX, R12

	// | w5 @ R10
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w11 @ R8
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ R12, R8

	// | w12 @ R9
	// | move to emptied register
	MOVQ 24(SP), R9
	ADCQ DI, R9
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 5				*/

	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ DX, R12

	// | w6 @ R11
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ $0x00, DX
	ADDQ R12, R11
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w11 @ R8
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w12 @ R9
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ R12, R9

	// | w13 @ R10
	// | move to emptied register
	MOVQ 32(SP), R10
	ADCQ DI, R10
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 6				*/

	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ DX, R12

	// | w7 @ R14
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ $0x00, DX
	ADDQ R12, R14
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w11 @ R8
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w12 @ R9
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w13 @ R10
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ R12, R10

	// | w14 @ R11
	// | move to emptied register
	MOVQ 40(SP), R11
	ADCQ DI, R11
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/*	i = 7				*/

	MOVQ R14, AX
	MULQ inp+32(FP)
	MOVQ AX, R13
	MOVQ $0x00, R12

	// |
	MOVQ (BX), AX
	MULQ R13
	ADDQ AX, R14
	ADCQ DX, R12

	// | w8 @ R15
	MOVQ 8(BX), AX
	MULQ R13
	ADDQ AX, R15
	ADCQ $0x00, DX
	ADDQ R12, R15
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w9 @ SI
	MOVQ 16(BX), AX
	MULQ R13
	ADDQ AX, SI
	ADCQ $0x00, DX
	ADDQ R12, SI
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w10 @ CX
	MOVQ 24(BX), AX
	MULQ R13
	ADDQ AX, CX
	ADCQ $0x00, DX
	ADDQ R12, CX
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w11 @ R8
	MOVQ 32(BX), AX
	MULQ R13
	ADDQ AX, R8
	ADCQ $0x00, DX
	ADDQ R12, R8
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w12 @ R9
	MOVQ 40(BX), AX
	MULQ R13
	ADDQ AX, R9
	ADCQ $0x00, DX
	ADDQ R12, R9
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w13 @ R10
	MOVQ 48(BX), AX
	MULQ R13
	ADDQ AX, R10
	ADCQ $0x00, DX
	ADDQ R12, R10
	MOVQ $0x00, R12
	ADCQ DX, R12

	// | w14 @ R11
	MOVQ 56(BX), AX
	MULQ R13
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ R12, R11

	// | w15 @ R14
	// | move to emptied register
	MOVQ 48(SP), R14
	ADCQ DI, R14
	MOVQ $0x00, DI
	ADCQ $0x00, DI

	// | 

/* reduction 				*/

	MOVQ R15, DX
	SUBQ (BX), DX
	MOVQ SI, R12
	SBBQ 8(BX), R12
	MOVQ CX, R13
	SBBQ 16(BX), R13
	MOVQ R8, AX
	SBBQ 24(BX), AX
	MOVQ AX, (SP)
	MOVQ R9, AX
	SBBQ 32(BX), AX
	MOVQ AX, 8(SP)
	MOVQ R10, AX
	SBBQ 40(BX), AX
	MOVQ AX, 16(SP)
	MOVQ R11, AX
	SBBQ 48(BX), AX
	MOVQ AX, 24(SP)
	MOVQ R14, AX
	SBBQ 56(BX), AX
	MOVQ AX, 32(SP)
	SBBQ $0x00, DI

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC DX, R15
	MOVQ    R15, (DI)
	CMOVQCC R12, SI
	MOVQ    SI, 8(DI)
	CMOVQCC R13, CX
	MOVQ    CX, 16(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 24(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 32(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 40(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 48(DI)
	CMOVQCC 32(SP), R14
	MOVQ    R14, 56(DI)
	RET

	// | 

/* end 				*/


// func mul_two_8(a *[8]uint64)
TEXT ·mul_two_8(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RET

// func div_two_8(a *[8]uint64)
TEXT ·div_two_8(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET
